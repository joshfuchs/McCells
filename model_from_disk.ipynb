{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4e25ff",
   "metadata": {},
   "source": [
    "Tutorial from https://chanzuckerberg.github.io/cellxgene-census/notebooks/experimental/pytorch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b822fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellxgene_census\n",
    "import cellxgene_census.experimental.ml as census_ml\n",
    "import tiledbsoma as soma\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torcheval.metrics.functional import multilabel_accuracy\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(style='whitegrid')\n",
    "sns.set_context(context='notebook')\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc(\n",
    "    'axes',\n",
    "    labelweight='bold',\n",
    "    labelsize='large',\n",
    "    titleweight='bold',\n",
    "    titlesize=9,\n",
    "    linewidth=4\n",
    "    )\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9355266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dba2fb",
   "metadata": {},
   "source": [
    "## Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed289f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(marginalization_dict,num_epochs,save_title=None):\n",
    "    fig, ax = plt.subplots(4,2,figsize=(9,12))\n",
    "    \n",
    "    \n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['accuracy_train_leaf_hist'], \n",
    "                    ax = ax[0,0],color='lightcoral',label='Train')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['accuracy_val_leaf_hist'], \n",
    "                    ax = ax[0,0],color='mediumslateblue',label='Validation')\n",
    "    \n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['accuracy_train_internal_hist'],\n",
    "                   ax = ax[0,1],color='lightcoral',label='Train')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['accuracy_val_internal_hist'],\n",
    "                   ax = ax[0,1],color='mediumslateblue',label='Validation')\n",
    "    \n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_train_hist'], \n",
    "                    ax = ax[1,0],color='lightcoral')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_val_hist'], \n",
    "                    ax = ax[1,0],color='mediumslateblue')\n",
    "    \n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_train_leaf_hist'], \n",
    "                    ax = ax[1,1],color='lightcoral',marker='X',label='Train Leafs')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_train_internal_hist'], \n",
    "                    ax = ax[1,1],color='lightcoral',marker='o',label='Train Internal')\n",
    "    #sns.scatterplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_train_internal_hist'], \n",
    "    #                ax = ax[1,0],color='lightcoral',marker='v',label='Train Internal')\n",
    "\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_val_leaf_hist'], \n",
    "                    ax = ax[2,0],color='mediumslateblue',marker='X',label='Val Leafs')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_val_internal_hist'], \n",
    "                    ax = ax[2,0],color='mediumslateblue',marker='o',label='Val Internal')\n",
    "    #sns.scatterplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_val_internal_hist'], \n",
    "    #                ax = ax[1,1],color='mediumslateblue',marker='v',label='Val Internal')\n",
    "\n",
    "\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['f1_score_train_leaf'], \n",
    "                    ax = ax[2,1],color='lightcoral',label='Train')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['f1_score_val_leaf'], \n",
    "                    ax = ax[2,1],color='mediumslateblue',label='Validation')\n",
    "\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['f1_score_train_internal'], \n",
    "                    ax = ax[3,0],color='lightcoral',label='Train')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['f1_score_val_internal'], \n",
    "                    ax = ax[3,0],color='mediumslateblue',label='Validation')\n",
    "\n",
    "    \n",
    "    ax[0,0].set_xlabel('Epoch')\n",
    "    ax[0,1].set_xlabel('Epoch')\n",
    "    ax[1,0].set_xlabel('Epoch')\n",
    "    ax[1,1].set_xlabel('Epoch')\n",
    "    ax[2,0].set_xlabel('Epoch')\n",
    "    ax[2,1].set_xlabel('Epoch')\n",
    "    ax[3,0].set_xlabel('Epoch')\n",
    "\n",
    "\n",
    "    ax[0,0].set_ylabel('Leaf Accuracy')\n",
    "    ax[0,1].set_ylabel('Internal Accuracy')\n",
    "    ax[1,0].set_ylabel('Total Loss')\n",
    "    ax[1,1].set_ylabel('Training Loss')\n",
    "    ax[2,0].set_ylabel('Validation Loss')\n",
    "    ax[2,1].set_ylabel('Leaf F1 Score')\n",
    "    ax[3,0].set_ylabel('Internal F1 Score')\n",
    "\n",
    "\n",
    "    # set the boundary for the accuracy plots\n",
    "    #ax[0,1].set_ylim((50,100))\n",
    "    \n",
    "    # turn off the axis for subplot 2,1\n",
    "    ax[3,1].axis('off')\n",
    "    \n",
    "    if save_title:\n",
    "        plt.savefig(save_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e491b4",
   "metadata": {},
   "source": [
    "## Load the Saved Outputs from McCell_preprocessing\n",
    "\n",
    "The preproccessing and modeling here should be run on the **same dataset**. If not, there might be differences in the ordering of cells the would nullify this model. \n",
    "\n",
    "- cell_parent_mask\n",
    "- Mapping_dict\n",
    "- Ontology_df\n",
    "- Internal_values\n",
    "- leaf_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531a2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing outputs are normally stored on Turbo\n",
    "os.chdir('/nfs/turbo/umms-welchjd/mccell/preprocessing_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the date in yyyy-mm-dd format\n",
    "date = '2024-03-18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea04d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_df_name = date + '_ontology_df.csv'\n",
    "ontology_df = pd.read_csv(ontology_df_name,index_col=0)\n",
    "\n",
    "\n",
    "mapping_dict_name = date + '_mapping_dict_df.csv'\n",
    "mapping_dict_df = pd.read_csv(mapping_dict_name,index_col=0)\n",
    "mapping_dict = mapping_dict_df.T.to_dict('list')\n",
    "# the values are stored as a list. convert to single value\n",
    "for key, value in mapping_dict.items():\n",
    "    mapping_dict[key] = value[0]\n",
    "\n",
    "leaf_values_name = date + '_leaf_values'\n",
    "internal_values_name = date + '_internal_values'\n",
    "with open(leaf_values_name,'rb') as fp:\n",
    "    leaf_values = pickle.load(fp)\n",
    "with open(internal_values_name,'rb') as fp:\n",
    "    internal_values = pickle.load(fp)\n",
    "\n",
    "\n",
    "cell_parent_mask_name = date + '_cell_parent_mask.pt'\n",
    "cell_parent_mask = torch.load(cell_parent_mask_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caceac8",
   "metadata": {},
   "source": [
    "## Build the Experiment and the DataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5274b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene and cell type info stored on Turbo\n",
    "os.chdir('/nfs/turbo/umms-welchjd/mccell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2d95a",
   "metadata": {},
   "source": [
    "First, let's load the gene list and cell type list that we want from the Census. Then we construct the ```var_val_filter``` and ```obs_val_filter``` for querying the census."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91823465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gene list\n",
    "biomart = pd.read_csv('mart_export.txt')\n",
    "\n",
    "coding_only = biomart[biomart['Gene type'] == 'protein_coding']\n",
    "\n",
    "gene_list = coding_only['Gene stable ID'].to_list()\n",
    "\n",
    "var_val_filter = '''feature_id in {}'''.format(gene_list)\n",
    "\n",
    "# load the cell type list\n",
    "cell_type_list_name = 'cell_type_list.txt'\n",
    "with open(cell_type_list_name,'rb') as fp:\n",
    "    cell_type_list = pickle.load(fp)\n",
    "\n",
    "obs_val_filter = '''assay == \"10x 3\\' v3\" and is_primary_data == True and cell_type_ontology_term_id in {}'''.format(cell_type_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0f26d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#organism = \"Homo sapiens\"\n",
    "col_names = {\"obs\": [\"cell_type_ontology_term_id\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "192a33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the Census and create the Experiment\n",
    "census = cellxgene_census.open_soma(uri = \"/scratch/welchjd_root/welchjd99/fujoshua/soma\")\n",
    "experiment = census[\"census_data\"][\"homo_sapiens\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "437e76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256 #128 #8192 # 4096\n",
    "soma_chunk_size = 50_000 #10_000\n",
    "\n",
    "experiment_datapipe = census_ml.ExperimentDataPipe(\n",
    "    experiment,\n",
    "    measurement_name=\"RNA\",\n",
    "    X_name=\"raw\",\n",
    "    obs_query=soma.AxisQuery(value_filter=obs_val_filter),\n",
    "    var_query=soma.AxisQuery(value_filter=var_val_filter),\n",
    "    obs_column_names=[\"cell_type_ontology_term_id\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    soma_chunk_size=soma_chunk_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54f2e94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2726029, 19966)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_datapipe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195167e",
   "metadata": {},
   "source": [
    "Split the datapipe into Train and Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1373237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.0012\n",
    "val_percent = 0.0008\n",
    "extra_percent = 0.999 # use this to reduce the size of the dataset\n",
    "\n",
    "# if you give random_split only two values, it normalizes those to the whole dataset, \n",
    "# i.e. if you give it 0.12 and 0.08 (in an attempt to train a subset of the dataset), \n",
    "# you get a 60/40 traing/validation split\n",
    "\n",
    "train_datapipe, val_datapipe = experiment_datapipe.random_split(weights={\"train\": train_percent, \"val\": val_percent},\n",
    "                                                                seed=42)\n",
    "#train_datapipe, val_datapipe, test_datapipe = experiment_datapipe.random_split(weights={\"train\": train_percent, \"test\": val_percent,'val':extra_percent},\n",
    "#                                                                seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f5e6e",
   "metadata": {},
   "source": [
    "Build the dataloaders for the train and test splits. We don't use PyTorch ```DataLoader``` directly because the ```ExperimentDataPipe``` already deals with the necessary parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b58334",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = census_ml.experiment_dataloader(train_datapipe)\n",
    "val_dataloader = census_ml.experiment_dataloader(val_datapipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fca5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd854341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66c1fc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IterDataPipeSerializationWrapper"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_dataloader.size\n",
    "#print(len(train_dataloader.dataset))\n",
    "train_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "453e07d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time 395.4807653427124\n"
     ]
    }
   ],
   "source": [
    "start_batch = time.time()\n",
    "for batch in train_dataloader:\n",
    "    print('running time', (time.time()-start_batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b38d1747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time 353.2778468132019\n"
     ]
    }
   ],
   "source": [
    "start_batch = time.time()\n",
    "for batch in val_dataloader:\n",
    "    print('running time', (time.time()-start_batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c8c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecae61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00ada2e8",
   "metadata": {},
   "source": [
    "## Build Neural Network Classifier\n",
    "\n",
    "First, we need to select and define the input and output dimensions from the data. The number of neurons for the hidden nodes is defined manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07043911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19966 256 128 88\n"
     ]
    }
   ],
   "source": [
    "# number of features (len of X cols)\n",
    "# select the number of gene columns\n",
    "input_dim = experiment_datapipe.shape[1]#X_train.size(dim=1) #adata.X.shape[1] \n",
    "\n",
    "# number of neurons for hidden layers\n",
    "hidden_layer_1 = 256\n",
    "hidden_layer_2 = 128\n",
    "\n",
    "# number of leaf classes (unique of y that are greater than or equal to 0)\n",
    "output_dim = len(leaf_values) #torch.unique(y_train[y_train >= 0]).size(dim=0) #labels['encoded_labels'].nunique()\n",
    "\n",
    "print(input_dim,hidden_layer_1,hidden_layer_2,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64bec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim,hidden_layer_1)\n",
    "        self.linear2 = nn.Linear(hidden_layer_1,hidden_layer_2)\n",
    "        self.linear3 = nn.Linear(hidden_layer_2,output_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_layer_1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_layer_2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.softmax(x,dim=1)\n",
    "        return x\n",
    "    \n",
    "    def get_last_layer(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf977d",
   "metadata": {},
   "source": [
    "## Functions for dealing with cell ontology and loss calculations\n",
    "\n",
    "We'll need a few specific functions to process the predicted values with the structure of the Cell Ontology. We'll define these here. Full details of each function are found in each space. \n",
    "\n",
    "- output_probability_tensor: convolves the predicted classification outputs with the ontology hierarchy to get predicted normalized probabilities for all parent nodes\n",
    "- target_probability_tensor: convolves the known target values with the ontology hierarchy to get target probabilities for all parent nodes\n",
    "- build_mask_tensor_for_batch : builds a masking tensor from cell_parent_mask specific to the targets for each batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a022561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(x_data):\n",
    "    '''\n",
    "    This function takes the input x_data, transforms the data with log(1+x) and \n",
    "    returns the transformed data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data : scipy matrix\n",
    "        scipy sparse CSR matrix  \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_data : SciPy Matrix\n",
    "        scipy sparse CSR matrix\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # np.log takes the natural log\n",
    "    x_data.data = np.log(1+ x_data.data)\n",
    "\n",
    "    return x_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83a3c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probability_tensor(outputs,ontology_df):\n",
    "    '''\n",
    "    Function to convolve the predicted classification outputs with the ontology heirarchy to\n",
    "    get predicted normalized probabilities for all parents. \n",
    "    Precursur to loss calculation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    outputs : tensor\n",
    "        PyTorch tensor of shape [a,b] where a = number of cells and b = number of target leafs\n",
    "        This tensor is the result of the classification in the neural network\n",
    "                \n",
    "    ontology_df : pandas dataframe\n",
    "        pandas dataframe where rows are parent labels and columns are leafs\n",
    "        values indicate if parent node is an ancestor of leaf node\n",
    "\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    sum_probability_tensor : tensor\n",
    "        PyTorch tensor of shape [c,a] where c = number of cell ontology IDs and a = number of cells\n",
    "        Each entry is the summed predicted probability for that cell ID\n",
    "\n",
    "    '''\n",
    "        \n",
    "    # convert the dataframe to a pytorch tensor\n",
    "    ontology_tensor = torch.FloatTensor(ontology_df.values)\n",
    "    ontology_tensor = ontology_tensor.to(device)\n",
    "        \n",
    "    # convolve the ontology tensor with the predicted outputs\n",
    "    # ontology tensor is shape ij, where i = parent IDs, j = probability for leaf IDs\n",
    "    # output tensor is shape kj, where k = number of cells classified, j = probability for leaf IDs\n",
    "    # probability tensor is shape ijk\n",
    "    \n",
    "    # if there is only a single column for the ontology, change shape to match expected value\n",
    "    if len(ontology_tensor.shape) == 1:\n",
    "        ontology_tensor = ontology_tensor.unsqueeze(1)    \n",
    "    #print(ontology_tensor.shape)\n",
    "    #print(outputs.shape)\n",
    "    probability_tensor = torch.einsum('ij,kj->ijk',ontology_tensor,outputs)\n",
    "    #print('prob_tensor', probability_tensor.shape)\n",
    "    \n",
    "    # sum across leafs to get the predicted probability, by cell, for each\n",
    "    # parent \n",
    "    # sum_probability_tensor is shape ik, where i = parent IDs, k = number of cells\n",
    "    sum_probability_tensor = torch.sum(probability_tensor,dim=1,dtype=float)\n",
    "    \n",
    "    ##sum_masked_probability_tensor = sum_probability_tensor * batch_masking_tensor\n",
    "    #print('sum masked',sum_masked_probability_tensor.shape)\n",
    "    #print('sum masked',sum_masked_probability_tensor.sum(dim=1))\n",
    "    # ensure that the max value is 1 because of floating point issues\n",
    "    # if we don't do this, we can run into errors with the binary cross entropy\n",
    "    ##sum_masked_probability_tensor = torch.where(sum_masked_probability_tensor > 1, 1.0, sum_masked_probability_tensor )\n",
    "    sum_probability_tensor = torch.where(sum_probability_tensor > 1, 1.0, sum_probability_tensor )\n",
    "\n",
    "    return sum_probability_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0e3aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask_tensor_for_batch(cell_parent_mask,y_batch,min_encoded_value,max_encoded_value):\n",
    "    '''\n",
    "    For each batch, this function builds the correct masking tensor based on which\n",
    "    values of the cell ontology we want to include given the target. It returns aa \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cell_parent_mask : tensor\n",
    "        tensor of shape ik, where i = parent IDs and k = each cell type in the dataset\n",
    "        binary tensor where 1 means for that cell type, that parent ID will be included\n",
    "        in the internal loss calculation\n",
    "        and 0 means for that cell type, that parent ID is excluded in the internal loss\n",
    "        calculation\n",
    "                \n",
    "    y_batch : tensor\n",
    "        tensor with encoded target values for current batch of data\n",
    "        \n",
    "    min_encoded_value : int\n",
    "        the minimum encoded value from the full set of target values. Typically -9999\n",
    "        \n",
    "    max_encoded_value : int\n",
    "        the maximum encoded value from the full set of target values. Depends on number\n",
    "        of leaf targets in the dataset\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    batch_masking_tensor : tensor\n",
    "        PyTorch tensor of shape [c,a] where c = number of cell ontology IDs and a = number of cells\n",
    "        Binary tensor to zero out probabilities we do not want\n",
    "    \n",
    "    '''\n",
    "    # 1) from y_batch, we need to get the indices we'll use to select from cell_parent_mask\n",
    "    #.   all the positive values are each, but we need to convert the negative values to\n",
    "    #.   correspond with the (positive) index they would otherwise be. Then save to a tensor\n",
    "    \n",
    "    for value in y_batch:\n",
    "        if value >= 0:\n",
    "            new_value = value\n",
    "        else:\n",
    "            new_value = (value - min_encoded_value) + max_encoded_value + 1\n",
    "        try:\n",
    "            converted_y_batch = torch.cat((converted_y_batch,new_value.reshape(1)),dim=0)\n",
    "        except:\n",
    "            converted_y_batch = new_value.reshape(1)\n",
    "    \n",
    "    \n",
    "    # 2) use the y_batch converted values to build a new tensor from cell_parent_mask\n",
    "    #.    that is the mask we will use for this batch of values.\n",
    "    #.    return this tensor\n",
    "\n",
    "    cell_parent_mask = cell_parent_mask.to(device)\n",
    "    batch_masking_tensor = torch.index_select(cell_parent_mask,1,converted_y_batch)\n",
    "    #print(batch_masking_tensor.sum(dim=0))\n",
    "    \n",
    "    return(batch_masking_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47cb3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_probability_tensor(target_values,ontology_df,mapping_dict):\n",
    "    '''\n",
    "    Function to convolve the known target values with the ontology heirarchy\n",
    "    Precursur to loss calculation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target_values : tensor\n",
    "        PyTorch tensor of shape [a] where a = number of cells\n",
    "                \n",
    "    ontology_df : pandas dataframe\n",
    "        pandas dataframe where rows are parent labels and columns are leafs\n",
    "        values indicate if parent node is an ancestor of leaf node\n",
    "\n",
    "    mapping_dict : dictionary\n",
    "        dictionary mapping the Cell Ontology IDs (keys) to the encoded values (values)\n",
    "        Values >= 0 are leaf nodes\n",
    "        Values < 0 are internal nodes\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    target_tensor : tensor\n",
    "        PyTorch tensor of shape [c,a] where c = number of cell ontology IDs and a = number of cells\n",
    "        Each entry is the summed predicted probability for that cell ID\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # loop through target values, pick out corresponding column of ontology_df\n",
    "    # append that to a tensor\n",
    "    \n",
    "    # invert the mapping dict so that we can select columns by CELL TYPE ID\n",
    "    inv_mapping_dict = {v: k for k,v in mapping_dict.items()}\n",
    "\n",
    "    for count, target_value in enumerate(target_values):\n",
    "        # get the cell ID from the inverted mapping dictionary based on the encoded value\n",
    "        target_cell_id = inv_mapping_dict[target_value.item()]\n",
    "        \n",
    "        # look up the correct column by the Cell ID. get those column values and convert\n",
    "        # to a tensor\n",
    "        sub_target_tensor = torch.tensor(ontology_df.loc[:,target_cell_id].values,dtype=float).reshape(-1,1)\n",
    "        \n",
    "        if count == 0 :\n",
    "            target_tensor = sub_target_tensor\n",
    "        else:\n",
    "            # set requires_grad so that we can track\n",
    "            target_tensor = torch.cat((target_tensor,sub_target_tensor),1).requires_grad_()\n",
    "    #print('target tensor shape',target_tensor.shape)\n",
    "    #print('batch_masking_tensor',batch_masking_tensor.shape)\n",
    "    ###masked_target_tensor = target_tensor * batch_masking_tensor\n",
    "    ##print('masked target tensor',masked_target_tensor.shape)\n",
    "    \n",
    "    target_tensor = target_tensor.to(device)\n",
    "    ###masked_target_tensor = masked_target_tensor.to(device)\n",
    "    \n",
    "    \n",
    "    return target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf08074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e85f3a8f",
   "metadata": {},
   "source": [
    "# Marginalization Classification \n",
    "\n",
    "- Based on Dahll, et al., Hierarchical Image Classification using Entailment Cone Embeddings, CVPR 202\n",
    "- https://arxiv.org/pdf/2004.03459.pdf\n",
    "- Thesis slides: https://ankitdhall.github.io/publication/learning-representations-for-images-with-hierarchical-labels/master_thesis.pdf\n",
    "- First Author website: https://ankitdhall.github.io/project/learning-representations-for-images-with-hierarchical-labels/\n",
    "\n",
    "Important Details:\n",
    "- we use mini-batch learning, with the batch size set by the user\n",
    "- we model each batch of data at once, then split into leaf and internal nodes, based on the values in y_batch\n",
    "- we calculate the loss two different ways, then sum to get the total loss\n",
    "- we calculate and save the loss, accuracy, and F1 score for metrics to review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9dae7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginalization_classification_manual_batch(train_dataloader,val_dataloader,num_epochs,ontology_leaf_df, \n",
    "                                                batch_size,internal_values,mapping_dict,\n",
    "                                               ontology_df, threshold, cell_parent_mask,encoding_mapper):\n",
    "    '''\n",
    "    Performs training and validation simultaneously to allow visualization of model performance \n",
    "    per epoch. Accounts for entire tree structure of the ontology by classifying to the leaf nodes, \n",
    "    propogating the probabilities across the ontology, and calculating the loss for both the leafs \n",
    "    and parent nodes. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_dataloader : DataLoader object\n",
    "        DataLoader object for training data from census_ml.experiment_dataloader for \n",
    "        streaming data from disk from the CZI cell_census API\n",
    "        similar to PyTorach DataLoader but specific to working with Cell Census data\n",
    "    \n",
    "    val_dataloader : DataLoader object\n",
    "        DataLoader object for validation data from census_ml.experiment_dataloader for \n",
    "        streaming data from disk from the CZI cell_census API\n",
    "        similar to PyTorach DataLoader but specific to working with Cell Census \n",
    "        \n",
    "    num_epochs : int\n",
    "        integer specify the number of epochs\n",
    "        \n",
    "    ontology_leaf_df : pandas dataframe\n",
    "        pandas dataframe where indices (rows) are all leaf and parent cell IDs from the portion of \n",
    "        the ontology being queried, and columns are onlys leafs in portion of ontology being queried. \n",
    "        Differs from ontology_df in that columns do not include any internal nodes.\n",
    "        \n",
    "        Dataframe is binary. For each parent node, element = 1 if parent node is an ancestor\n",
    "        of corresponding leaf node.\n",
    "    \n",
    "    batch_size : int\n",
    "        integer specificying the number of samples processed before the model\n",
    "        is updated\n",
    "        \n",
    "    internal_values : list\n",
    "        list of Cell Ontology IDs for internal nodes included in the dataset\n",
    "        \n",
    "    mapping_dict : dict\n",
    "        dictionary mapping the Cell Ontology IDs (keys) to the encoded values (values)\n",
    "        Values >= 0 are leaf nodes\n",
    "        Values < 0 are internal nodes\n",
    "    \n",
    "    ontology_df : pandas dataframe\n",
    "        pandas dataframe where indices (rows) are all leaf and parent cell IDs from the portion of \n",
    "        the ontology being queried, and columns are leafs and internal nodes in portion of ontology being \n",
    "        queried. \n",
    "        Differs from ontology_leaf_df in that columns include both leaf and internal node values\n",
    "        \n",
    "        Dataframe is binary. For each parent node, element = 1 if parent node is an ancestor\n",
    "        of corresponding leaf node.\n",
    "        \n",
    "    threshold : float\n",
    "        value between 0 and 1 to set for making predictions. If the predicted probability is\n",
    "        equal to or greater than threshold, we consider that a true prediction\n",
    "        \n",
    "    cell_parent_mask : tensor\n",
    "        tensor of shape ik, where i = parent IDs and k = each cell type in the dataset\n",
    "        binary tensor where 1 means for that cell type, that parent ID will be included\n",
    "        in the internal loss calculation\n",
    "        and 0 means for that cell type, that parent ID is excluded in the internal loss\n",
    "        calculation\n",
    "\n",
    "    encoding_mapper : dictionary\n",
    "        dictionary mapping from encoder_mapping_dict (keys) to mapping_dict (values)\n",
    "        used to differentiate leaf values and internal values \n",
    "        \n",
    "    Returns\n",
    "    -------    \n",
    "    marginalization_dict : dictionary\n",
    "        dictionary containing results from each epoch of the neural network\n",
    "    \n",
    "    Keys and Values include:\n",
    "    \n",
    "        accuracy_train_leaf_hist : list\n",
    "            list containing accuracy for leaf values for the training set per epoch\n",
    "            \n",
    "        accuracy_train_internal_hist : list\n",
    "            list containing accuracy for internal values for the trainig set per epoch\n",
    "        \n",
    "        loss_train_hist: list\n",
    "            list containing total loss values for the training set per epoch\n",
    "            \n",
    "        loss_train_leaf_hist : list\n",
    "            list containing loss values for leaf nodes for the training set per epoch\n",
    "            \n",
    "        loss_train_parents_hist : list\n",
    "            list containing loss values for parent nodes for the training set per epoch\n",
    "        \n",
    "        loss_train_internal_hist : list\n",
    "            list containing loss values for internal nodes for the training set per epoch\n",
    "        \n",
    "        accuracy_val_leaf_hist : list \n",
    "            list containing accuracy for leaf values for the validation set per epoch\n",
    "        \n",
    "        accuracy_val_internal_hist : list\n",
    "            list containing accuracy for internal values for the validation set per epoch\n",
    "        \n",
    "        loss_val_hist : list\n",
    "            list containing total loss values for the validation set per epoch\n",
    "            \n",
    "        loss_val_leaf_hist : list\n",
    "            list containing loss values for leaf nodes for the validation set per epoch\n",
    "            \n",
    "        loss_val_parents_hist : list\n",
    "            list containing loss values for parent nodes for the validation set per epoch\n",
    "            \n",
    "        loss_val_internal_hist : list\n",
    "            list containing loss values for internal nodes for the validation set per epoch\n",
    "        \n",
    "        f1_score_train_leaf : list\n",
    "            list containing Macro F1 score for leaf nodes for training set per epoch \n",
    "        \n",
    "        f1_score_val_leaf : list\n",
    "            list containing Macro F1 score for leaf nodes for validation set per epoch \n",
    "        \n",
    "        best_output : tensor\n",
    "            PyTorch tensor containing the predicted probabilites for the most accurate\n",
    "            epoch\n",
    "            \n",
    "        best_state_dict : dictionary\n",
    "            Pytorch state_dict that contains the parameters for the best fitting models\n",
    "    '''\n",
    "    # initialize variables for saving values\n",
    "    accuracy_train_leaf_hist = []\n",
    "    accuracy_train_internal_hist = []\n",
    "    loss_train_leaf_hist = []\n",
    "    loss_train_parents_hist = []\n",
    "    loss_train_internal_hist = []\n",
    "    loss_train_hist = []\n",
    "    \n",
    "    accuracy_val_leaf_hist = []\n",
    "    accuracy_val_internal_hist = []\n",
    "    loss_val_hist = []\n",
    "    loss_val_leaf_hist = []\n",
    "    loss_val_parents_hist = []\n",
    "    loss_val_internal_hist = []\n",
    "    \n",
    "    f1_score_train_leaf = []\n",
    "    f1_score_val_leaf = []\n",
    "    \n",
    "    f1_score_train_parent = []\n",
    "    f1_score_val_parent = []\n",
    "\n",
    "    best_accuracy = - np.inf\n",
    "    best_weights = None\n",
    "    \n",
    "    # get the list of leaf labels\n",
    "    leaf_label_list = [value for (key,value) in mapping_dict.items() if value >= 0]\n",
    "\n",
    "    # get the min and max encoded values\n",
    "    min_encoded_value = min(mapping_dict.values()) #min(y_train).item()\n",
    "    max_encoded_value = max(mapping_dict.values()) #max(y_train).item()\n",
    "\n",
    "    # initialize network\n",
    "    clf = Network()\n",
    "    clf.to(device)\n",
    "\n",
    "    # define loss and optimizer\n",
    "    # we use two different loss methods for the leafs and parents\n",
    "    # use Cross Entory Loss for leafs, because those probabilities are normalized\n",
    "    #     and it is thus a multi-class problem\n",
    "    # Use BCELoss for the parents because this is a multi-label problem\n",
    "    #     and the probabilities are normalized, so we don't need BCELossWithLogits\n",
    "    # initialize the leaf loss here\n",
    "    # because of how we weight the parent loss, we will have to initialize that\n",
    "    # loss on each iteration because the weighting will change.\n",
    "    criterion_leafs = nn.CrossEntropyLoss(reduction='mean')\n",
    "    \n",
    "    #criterion_parents = nn.BCELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(clf.parameters(), lr=1e-3)#, amsgrad=True, eps=1e-5)\n",
    "    #scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=5)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5,gamma=0.5)\n",
    "\n",
    "    #start_epoch = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        #print('on epoch', epoch)\n",
    "        \n",
    "        # TRAINING\n",
    "        #print('Begin Training...')\n",
    "        clf.train()\n",
    "        \n",
    "        running_train_loss = 0.0\n",
    "        #correct_train = 0\n",
    "        #y_length = 0\n",
    "                \n",
    "        # set up manual batches - from https://stackoverflow.com/questions/45113245/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way\n",
    "        #permutation = torch.randperm(X_train.size()[0]).to(device)\n",
    "        #permutation_cpu = permutation.cpu() # we want the same permutations, but need one copy on the cpu\n",
    "\n",
    "        print('start train batching')\n",
    "        start_batch = time.time()\n",
    "        #for i in range(0,X_train.size()[0], batch_size):\n",
    "        for i, train_batch in enumerate(train_dataloader):\n",
    "            #if (i/batch_size) % 10 == 0:\n",
    "            print('on batch', i, 'running time', (time.time()-start_batch))\n",
    "            #print(i)\n",
    "            #indices = permutation[i:i+batch_size]\n",
    "            #indices_cpu = permutation_cpu[i:i+batch_size]\n",
    "            #X_batch, y_batch = X_train[indices], y_train[indices] # doesn't work for sparse tensors\n",
    "\n",
    "            #X_batch = torch.index_select(X_train,0,indices_cpu).to(device)\n",
    "\n",
    "            #y_batch = torch.index_select(y_train,0,indices)#.to(device)\n",
    "\n",
    "            X_batch, y_batch = train_batch\n",
    "            \n",
    "            # change dtype to float\n",
    "            X_batch = X_batch.float()\n",
    "            \n",
    "            # transform the data with log(1+x)\n",
    "            X_batch = transform_data(X_batch)\n",
    "            \n",
    "            # move to device\n",
    "            X_batch = X_batch.to(device)\n",
    "\n",
    "            \n",
    "            # select the encoded values from the experiment_datapipe\n",
    "            y_batch = y_batch[:,1]\n",
    "            #print(y_batch)\n",
    "\n",
    "            # then map the values from the datapipe encoded values to the\n",
    "            # encoded values from the Ontology/mapping_dict\n",
    "            \n",
    "            #encoding_mapper\n",
    "            y_batch = torch.tensor([encoding_mapper[x.item()] for x in y_batch])\n",
    "            \n",
    "            # move to device\n",
    "            y_batch = y_batch.to(device)\n",
    "            #print(y_batch)\n",
    "            \n",
    "            # check that tensors are on gpu. if on gpu, get_device returns 0, if on cpu, returns -1\n",
    "            #print(X_batch.get_device())\n",
    "            #print(y_batch.get_device())\n",
    "            \n",
    "            # set optimizer to zero grad to remove previous epoch gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # make predictions for this batch\n",
    "            #if epoch == 0:\n",
    "            #    outputs_train = clf_nosoftmax(X_batch.float()) # might need to change to X_train.float()\n",
    "            #else:\n",
    "            outputs_train = clf(X_batch) # might need to change to X_train.float()\n",
    "            \n",
    "            ######\n",
    "            # create mask to separate leaf and internal nodes\n",
    "            ######\n",
    "            output_train_leaf = outputs_train[y_batch >= 0]\n",
    "            #print(output_train_leaf.shape)\n",
    "            y_batch_leaf = y_batch[y_batch >= 0]\n",
    "            #print(y_batch_leaf.shape)\n",
    "            \n",
    "            #output_train_internal = outputs_train[y_batch < 0]\n",
    "            #y_batch_internal = y_batch[y_batch < 0]\n",
    "            \n",
    "\n",
    "            # calculate loss for just the leafs\n",
    "            loss_train_leafs = criterion_leafs(output_train_leaf, y_batch_leaf)\n",
    "\n",
    "            # get the masking tensor for this batch of cells\n",
    "            # for calculating the internal loss\n",
    "            # we initialize BCE loss every batch because the mask changes based on which cells are\n",
    "            # included and ordered for this batch\n",
    "            batch_train_masking_tensor = build_mask_tensor_for_batch(cell_parent_mask,y_batch,min_encoded_value,max_encoded_value)\n",
    "            criterion_parents = nn.BCELoss(weight=batch_train_masking_tensor,reduction='mean')\n",
    "\n",
    "            \n",
    "            # calculate the loss for the parents of the cells that are leafs\n",
    "            output_train_parent_prob = output_probability_tensor(outputs_train,ontology_leaf_df)\n",
    "            target_train_parent_prob = target_probability_tensor(y_batch,ontology_df,mapping_dict)\n",
    "\n",
    "            #print(output_train_parent_prob)\n",
    "            #print('output_train_parent_prob',output_train_parent_prob.shape)\n",
    "            #print(target_train_parent_prob)\n",
    "            #print('target_train_parent_prob',target_train_parent_prob.shape)\n",
    "            \n",
    "            loss_train_parents = criterion_parents(output_train_parent_prob,target_train_parent_prob)\n",
    "\n",
    "            #######\n",
    "            # calculate the loss for the cells that are internal nodes\n",
    "            ##### total_accuracy_cell, total_number_of_cells\n",
    "            \n",
    "            #loss_train_internal, batch_accuracy_internal, batch_numbers_internal = internal_node_loss(\n",
    "            #                                        output_train_internal,y_batch_internal,\n",
    "            #                                         internal_values,mapping_dict,ontology_df,\n",
    "            #                                        ontology_leaf_df,criterion_parents,threshold)\n",
    "            \n",
    "            # sum the loss for both leafs and parents\n",
    "            loss_train = loss_train_leafs + loss_train_parents #+ loss_train_internal\n",
    "\n",
    "            # backward propagation\n",
    "            loss_train.backward()\n",
    "\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "             \n",
    "            running_train_loss += loss_train.item()\n",
    "           \n",
    "            # save predictions\n",
    "            _, train_leaf_pred_per_epoch = output_train_leaf.max(dim=1)\n",
    "\n",
    "            # calculate accuracy for internal cells\n",
    "            #####\n",
    "            # need to update this with the weighting somehow!!!!\n",
    "            #####\n",
    "            train_batch_accuracy = multilabel_accuracy(output_train_parent_prob,target_train_parent_prob,\n",
    "                                                      threshold=threshold,criteria='hamming')\n",
    "            \n",
    "            # save the number of cells for batch, for use in weighting when\n",
    "            # determining the overall accuracy per epoch\n",
    "            train_batch_number_of_cells = output_train_parent_prob.shape[1]\n",
    "                        \n",
    "            # check size of internal tensors. if only 1 internal cell\n",
    "            # reshape and detach\n",
    "            # else just detach\n",
    "            \n",
    "            ##if len(batch_accuracy_internal.size()) == 0:\n",
    "            ##    batch_accuracy_internal_shaped = batch_accuracy_internal.detach().reshape(1)\n",
    "            ##    batch_numbers_internal_shaped = batch_numbers_internal.detach().reshape(1)\n",
    "            ##else:\n",
    "            ##    batch_accuracy_internal_shaped = batch_accuracy_internal.detach()\n",
    "            ##    batch_numbers_internal_shaped = batch_numbers_internal.detach()\n",
    "                \n",
    "            \n",
    "            if i == 0:\n",
    "                train_leaf_pred_total = train_leaf_pred_per_epoch.detach()\n",
    "                y_train_leaf_total = y_batch_leaf.detach()\n",
    "                train_batch_accuracy_internal = train_batch_accuracy.reshape(1)\n",
    "                train_total_number_of_cells = torch.tensor(train_batch_number_of_cells).reshape(1)\n",
    "                output_train_probabilities = output_train_leaf.detach()\n",
    "                train_parent_pred_total = output_train_parent_prob.detach()\n",
    "                train_parent_true_total = target_train_parent_prob.detach()\n",
    "            else:\n",
    "                train_leaf_pred_total = torch.cat((train_leaf_pred_total,train_leaf_pred_per_epoch.detach()),0)\n",
    "                y_train_leaf_total = torch.cat((y_train_leaf_total,y_batch_leaf.detach()),0)\n",
    "                train_batch_accuracy_internal = torch.cat((train_batch_accuracy_internal,train_batch_accuracy.reshape(1)),0)\n",
    "                train_total_number_of_cells = torch.cat((train_total_number_of_cells,torch.tensor(train_batch_number_of_cells).reshape(1)),0)\n",
    "                output_train_probabilities = torch.cat((output_train_probabilities,output_train_leaf.detach()),0)\n",
    "                train_parent_pred_total = torch.cat((train_parent_pred_total,output_train_parent_prob.detach()),1)\n",
    "                train_parent_true_total = torch.cat((train_parent_true_total,target_train_parent_prob.detach()),1)\n",
    "                #print('train parent pred',train_parent_pred_total.shape)\n",
    "                #print('train parent true',train_parent_true_total.shape)\n",
    "                \n",
    "            # exit after 1 loop so we can test validation code\n",
    "            break\n",
    "                            \n",
    "            # calculate total epoch accuracy for internal nodes\n",
    "            #epoch_internal_accuracy = / train_numbers_internal.sum() * 100\n",
    "            \n",
    "            #correct_train += (train_pred_per_epoch == y_batch).sum().item()\n",
    "            #y_length += len(y_batch)\n",
    "            \n",
    "            # calculate F1 score\n",
    "            #f1_val_score_epoch = f1_score(train_pred_per_epoch.cpu(),y_batch.cpu(),labels=output_dim,average='weighted',zero_division=np.nan)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        print('done with training')\n",
    "        \n",
    "        # save tensors\n",
    "        #if epoch == 14:\n",
    "        #    torch.save(output_train_probabilities, 'output_good_nosoftmax_1Dec.pt')\n",
    "        #    torch.save(y_train_leaf_total, 'targets_good_nosoftmax_1Dec.pt')\n",
    "        \n",
    "        # save accuracy\n",
    "        #_, train_pred = outputs_train.max(dim=1)\n",
    "        #correct_train = (train_pred == y_train).sum().item()\n",
    "        #accuracy_train_hist.append(correct_train / y_length * 100.)\n",
    "        #print('acc train hist', accuracy_train_hist[-1])\n",
    "        \n",
    "        train_total_number_of_cells = train_total_number_of_cells.to(device)\n",
    "        \n",
    "        correct_train_leaf = (train_leaf_pred_total == y_train_leaf_total).sum().item()\n",
    "        print(train_leaf_pred_total.shape)\n",
    "        accuracy_train_leaf_hist.append(correct_train_leaf / train_leaf_pred_total.shape[0] * 100.)\n",
    "                \n",
    "        correct_train_internal = (train_batch_accuracy_internal * train_total_number_of_cells).sum()\n",
    "        accuracy_train_internal = (correct_train_internal / train_total_number_of_cells.sum() * 100.).item()\n",
    "        accuracy_train_internal_hist.append(accuracy_train_internal)\n",
    "        \n",
    "        #print('sample acc', acc_full)\n",
    "\n",
    "        # save loss\n",
    "        loss_train_hist.append(loss_train.item())\n",
    "        loss_train_leaf_hist.append(loss_train_leafs.item())\n",
    "        loss_train_parents_hist.append(loss_train_parents.item())\n",
    "        ##loss_train_internal_hist.append(loss_train_internal.item())\n",
    "        \n",
    "        # save f1 score\n",
    "        # use average = weighted to account for label imbalance\n",
    "        # use zero_division = np.nan to exclude labels where all \n",
    "        #       predictions and labels are negative\n",
    "        f1_train_leaf_score = f1_score(y_train_leaf_total.cpu(), train_leaf_pred_total.cpu(),\n",
    "                                  labels=leaf_label_list,average='weighted',zero_division=np.nan)\n",
    "        f1_score_train_leaf.append(f1_train_leaf_score)\n",
    "        \n",
    "        # for the F1 score for the internal nodes, we need to first turn the probabilities\n",
    "        # into predictions using our threshold value\n",
    "        train_parent_pred_total_thresholded = torch.where(train_parent_pred_total > threshold,1.0,0.0)\n",
    "        \n",
    "        f1_train_parent_score = f1_score(train_parent_true_total.cpu(),train_parent_pred_total_thresholded.cpu(),\n",
    "                                        average='weighted',zero_division=np.nan)\n",
    "        f1_score_train_parent.append(f1_train_parent_score)\n",
    "        #torch.save(train_parent_true_total.cpu(),'train_parent_true_total_20Feb.pt')\n",
    "        #torch.save(train_parent_pred_total_thresholded.cpu(),'train_parent_pred_total_thresholded_20Feb.pt')\n",
    "\n",
    "        \n",
    "        # set up validation\n",
    "        correct_val = 0\n",
    "        y_val_length = 0\n",
    "        \n",
    "        print('start validation')\n",
    "        with torch.no_grad():\n",
    "            clf.eval()\n",
    "            \n",
    "            # set up manual batches\n",
    "            # we don't need to randomly permute the validation set, but\n",
    "            # this will provide consistency with the above.\n",
    "            # for simplicity, let's use the same batch size\n",
    "            #permutation_val = torch.randperm(X_val.size()[0]).to(device)\n",
    "            #start_val = time.time()\n",
    "            \n",
    "            #for i in range(0,X_val.size()[0],batch_size):\n",
    "            print('start validation batching')\n",
    "            start_batch = time.time()\n",
    "            for i, val_batch in enumerate(val_dataloader):\n",
    "                print('on batch', i, 'running time', (time.time()-start_batch))\n",
    "\n",
    "                #indices_val = permutation_val[i:i+batch_size]\n",
    "                \n",
    "                #X_val_batch = torch.index_select(X_val,0,indices_val)\n",
    "                #y_val_batch = torch.index_select(y_val,0,indices_val)\n",
    "                \n",
    "                \n",
    "                X_val_batch, y_val_batch = val_batch\n",
    "            \n",
    "                # change dtype to float\n",
    "                X_val_batch = X_val_batch.float()\n",
    "\n",
    "                # transform the data with log(1+x)\n",
    "                X_val_batch = transform_data(X_val_batch)\n",
    "                \n",
    "                # move to device\n",
    "                X_val_batch = X_val_batch.to(device)\n",
    "\n",
    "                \n",
    "                # select the encoded values from the experiment_datapipe\n",
    "                y_val_batch = y_val_batch[:,1]\n",
    "                #print(y_batch)\n",
    "\n",
    "                # then map the values from the datapipe encoded values to the\n",
    "                # encoded values from the Ontology/mapping_dict\n",
    "\n",
    "                #encoding_mapper\n",
    "                y_val_batch = torch.tensor([encoding_mapper[x.item()] for x in y_val_batch])\n",
    "                \n",
    "                # move to device\n",
    "                y_val_batch = y_val_batch.to(device)\n",
    "                \n",
    "                # calculate output by running through the network\n",
    "                outputs_val = clf(X_val_batch)\n",
    "            \n",
    "                ######\n",
    "                # create mask to separate leaf and internal nodes\n",
    "                ######\n",
    "                output_val_leaf = outputs_val[y_val_batch >= 0]\n",
    "                y_val_batch_leaf = y_val_batch[y_val_batch >= 0]\n",
    "\n",
    "                #output_val_internal = outputs_val[y_val_batch < 0]\n",
    "                #y_val_batch_internal = y_val_batch[y_val_batch < 0]\n",
    "            \n",
    "                # calculate loss for just the leafs\n",
    "                loss_val_leafs = criterion_leafs(output_val_leaf, y_val_batch_leaf)\n",
    "        \n",
    "                # get the masking tensor for this batch of cells\n",
    "                # for calculating the internal loss\n",
    "                # we initialize BCE loss every batch because the mask changes based on which cells are\n",
    "                # included and ordered for this batch\n",
    "                batch_val_masking_tensor = build_mask_tensor_for_batch(cell_parent_mask,y_val_batch,min_encoded_value,max_encoded_value)\n",
    "                criterion_parents = nn.BCELoss(weight=batch_val_masking_tensor,reduction='mean')\n",
    "                \n",
    "                # calculate the loss for the parents of the leafs\n",
    "                output_val_parent_prob = output_probability_tensor(outputs_val,ontology_leaf_df)\n",
    "                target_val_parent_prob = target_probability_tensor(y_val_batch,ontology_df,mapping_dict)\n",
    "                \n",
    "                print(output_val_parent_prob)\n",
    "                print(target_val_parent_prob)\n",
    "                loss_val_parents = criterion_parents(output_val_parent_prob,target_val_parent_prob)\n",
    "                print(loss_val_parents)\n",
    "                #######\n",
    "                # calculate the loss for the cells that are internal nodes\n",
    "                #####\n",
    "\n",
    "                #loss_val_internal, batch_accuracy_internal_val, batch_numbers_internal_val = internal_node_loss(output_val_internal,\n",
    "                #                                                y_val_batch_internal,\n",
    "                #                                                internal_values,mapping_dict,ontology_df,ontology_leaf_df,\n",
    "                #                                                criterion_parents,threshold)\n",
    "\n",
    "                \n",
    "                # sum the loss for both leafs and parents\n",
    "                loss_val = loss_val_leafs + loss_val_parents #+ loss_val_internal\n",
    "            \n",
    "            \n",
    "                # get the predictions\n",
    "                __, predicted_leaf_val_per_epoch = output_val_leaf.max(dim=1)            \n",
    "\n",
    "                # calculate accuracy for internal cells\n",
    "                #####\n",
    "                # need to update this with the weighting somehow!!!!\n",
    "                #####\n",
    "                val_batch_accuracy = multilabel_accuracy(output_val_parent_prob,target_val_parent_prob,\n",
    "                                                          threshold=threshold,criteria='hamming')\n",
    "\n",
    "                # save the number of cells for batch, for use in weighting when\n",
    "                # determining the overall accuracy per epoch\n",
    "                val_batch_number_of_cells = output_val_parent_prob.shape[1]\n",
    "\n",
    "                \n",
    "                # save accuracy\n",
    "                #correct_val += (predicted_val_per_epoch == y_val_batch).sum().item()\n",
    "                #y_val_length += len(y_val_batch)\n",
    "                \n",
    "                # check size of internal tensors. if only 1 internal cell\n",
    "                # reshape and detach\n",
    "                # else just detach\n",
    "                            \n",
    "                #if len(batch_accuracy_internal_val.size()) == 0:\n",
    "                #    batch_accuracy_internal_val_shaped = batch_accuracy_internal_val.detach().reshape(1)\n",
    "                #    batch_numbers_internal_val_shaped = batch_numbers_internal_val.detach().reshape(1)\n",
    "                #else:\n",
    "                #    batch_accuracy_internal_val_shaped = batch_accuracy_internal_val.detach()\n",
    "                #    batch_numbers_internal_val_shaped = batch_numbers_internal_val.detach()\n",
    "                                \n",
    "                if i == 0:\n",
    "                    val_leaf_pred_total = predicted_leaf_val_per_epoch.detach()\n",
    "                    y_leaf_val_total = y_val_batch_leaf.detach()\n",
    "                    val_batch_accuracy_internal = val_batch_accuracy.reshape(1)\n",
    "                    val_total_number_of_cells = torch.tensor(val_batch_number_of_cells).reshape(1)\n",
    "                    val_parent_pred_total = output_val_parent_prob.detach()\n",
    "                    val_parent_true_total = target_val_parent_prob.detach()\n",
    "\n",
    "                else:\n",
    "                    val_leaf_pred_total = torch.cat((val_leaf_pred_total,predicted_leaf_val_per_epoch.detach()),0)\n",
    "                    y_leaf_val_total = torch.cat((y_leaf_val_total,y_val_batch_leaf.detach()),0)\n",
    "                    val_batch_accuracy_internal = torch.cat((val_batch_accuracy_internal,val_batch_accuracy.reshape(1)),0)\n",
    "                    val_total_number_of_cells = torch.cat((val_total_number_of_cells,torch.tensor(val_batch_number_of_cells).reshape(1)),0)\n",
    "                    val_parent_pred_total = torch.cat((val_parent_pred_total,output_val_parent_prob.detach()),1)\n",
    "                    val_parent_true_total = torch.cat((val_parent_true_total,target_val_parent_prob.detach()),1)\n",
    "\n",
    "                    \n",
    "                # exit after 1 loop so we can test  code\n",
    "                break\n",
    "\n",
    "            # save total accuracy\n",
    "            #accuracy_val_hist.append(correct_val / y_val_length * 100.)\n",
    "            print('done with validation')\n",
    "            val_total_number_of_cells = val_total_number_of_cells.to(device)\n",
    "            \n",
    "            correct_val_leaf = (val_leaf_pred_total == y_leaf_val_total).sum().item()\n",
    "            print(val_leaf_pred_total.shape)\n",
    "            accuracy_val_leaf_hist.append(correct_val_leaf / val_leaf_pred_total.shape[0] * 100.)\n",
    "\n",
    "            correct_val_internal = (val_batch_accuracy_internal * val_total_number_of_cells).sum()\n",
    "            accuracy_val_internal = (correct_val_internal / val_total_number_of_cells.sum() * 100.).item()\n",
    "            accuracy_val_internal_hist.append(accuracy_val_internal)\n",
    "\n",
    "            \n",
    "            # save loss\n",
    "            loss_val_hist.append(loss_val.item())\n",
    "            loss_val_leaf_hist.append(loss_val_leafs.item())\n",
    "            loss_val_parents_hist.append(loss_val_parents.item())\n",
    "            #loss_val_internal_hist.append(loss_val_internal.item())\n",
    "            \n",
    "            # save f1 score\n",
    "            # use average = weighted to account for label imbalance\n",
    "            # use zero_division = np.nan to exclude labels where all \n",
    "            #       predictions and labels are negative\n",
    "            f1_val_leaf_score = f1_score(y_leaf_val_total.cpu(),val_leaf_pred_total.cpu(),\n",
    "                                    labels=leaf_label_list,average='weighted',zero_division=np.nan)\n",
    "            f1_score_val_leaf.append(f1_val_leaf_score)\n",
    "\n",
    "            \n",
    "            # for the F1 score for the internal nodes, we need to first turn the probabilities\n",
    "            # into predictions using our threshold value\n",
    "            val_parent_pred_total_thresholded = torch.where(val_parent_pred_total > threshold,1.0,0.0)\n",
    "\n",
    "            f1_val_parent_score = f1_score( val_parent_true_total.cpu(),val_parent_pred_total_thresholded.cpu(),\n",
    "                                        average='weighted',zero_division=np.nan)\n",
    "            f1_score_val_parent.append(f1_val_parent_score)\n",
    "            #torch.save(val_parent_true_total.cpu(),'val_parent_true_total_20Feb.pt')\n",
    "            #torch.save(val_parent_pred_total_thresholded.cpu(),'val_parent_pred_total_thresholded_20Feb.pt')\n",
    "\n",
    "            \n",
    "            # check if best model\n",
    "            if accuracy_val_leaf_hist[-1] > best_accuracy:\n",
    "                best_acc = accuracy_val_leaf_hist[-1]\n",
    "                best_state_dict = copy.deepcopy(clf.state_dict())\n",
    "                best_output = copy.deepcopy(outputs_val)\n",
    "            \n",
    "        if (epoch + 1) % 1 == 0 or epoch == 0:\n",
    "            print(f'[{epoch + 1}] Training Accuracy: {accuracy_train_leaf_hist[-1]:.3f} Validation Accuracy: {accuracy_val_leaf_hist[-1]:.3f}')\n",
    "            print(f'Train Loss: {loss_train.item():.4f} Validation Loss: {loss_val.item():.4f}')\n",
    "            #print(f'Internal Loss: {loss_val_internal.item():.4f}')\n",
    "            #print('learning rate:', optimizer.param_groups[0][\"lr\"])\n",
    "        #end_epoch = time.time()\n",
    "        #print('epoch timer', end_epoch-start_epoch)\n",
    "        #break\n",
    "\n",
    "    print(f'Best Validation Accuracy: {best_acc:.3f}')\n",
    "    \n",
    "    # build dictionary to return values\n",
    "    marginalization_dict = {}\n",
    "    marginalization_dict['accuracy_train_leaf_hist'] = accuracy_train_leaf_hist\n",
    "    marginalization_dict['accuracy_train_internal_hist'] = accuracy_train_internal_hist\n",
    "    \n",
    "    marginalization_dict['loss_train_hist'] = loss_train_hist\n",
    "    \n",
    "    marginalization_dict['loss_train_leaf_hist'] = loss_train_leaf_hist\n",
    "    marginalization_dict['loss_train_internal_hist'] = loss_train_parents_hist\n",
    "\n",
    "    marginalization_dict['accuracy_val_leaf_hist'] = accuracy_val_leaf_hist\n",
    "    marginalization_dict['accuracy_val_internal_hist'] = accuracy_val_internal_hist\n",
    "\n",
    "    marginalization_dict['loss_val_hist'] = loss_val_hist\n",
    "\n",
    "    marginalization_dict['loss_val_leaf_hist'] = loss_val_leaf_hist\n",
    "    marginalization_dict['loss_val_internal_hist'] = loss_val_parents_hist\n",
    "    \n",
    "    marginalization_dict['f1_score_train_leaf'] = f1_score_train_leaf\n",
    "    marginalization_dict['f1_score_val_leaf'] = f1_score_val_leaf\n",
    "    \n",
    "    marginalization_dict['f1_score_train_internal'] = f1_score_train_parent\n",
    "    marginalization_dict['f1_score_val_internal'] = f1_score_val_parent\n",
    "\n",
    "    marginalization_dict['best_output'] = best_output\n",
    "    marginalization_dict['best_state_dict'] = best_state_dict\n",
    "\n",
    "\n",
    "    return marginalization_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0768f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 88 leafs and 55 parents.\n",
      "Number of training batches is 12.78\n",
      "Number of validation batches is 8.52\n",
      "Prediction threshold for internal nodes is 0.1\n",
      "start train batching\n",
      "on batch 0 running time 223.66569828987122\n",
      "done with training\n",
      "torch.Size([1])\n",
      "start validation\n",
      "start validation batching\n",
      "on batch 0 running time 398.92136788368225\n",
      "tensor([[0.0391, 0.0358, 0.0374,  ..., 0.0380, 0.0370, 0.0366],\n",
      "        [0.0627, 0.0511, 0.0541,  ..., 0.0546, 0.0520, 0.0519],\n",
      "        [0.0170, 0.0134, 0.0138,  ..., 0.0145, 0.0135, 0.0133],\n",
      "        ...,\n",
      "        [0.0423, 0.0447, 0.0462,  ..., 0.0460, 0.0466, 0.0463],\n",
      "        [0.0442, 0.0368, 0.0370,  ..., 0.0383, 0.0355, 0.0358],\n",
      "        [0.0469, 0.0541, 0.0539,  ..., 0.0534, 0.0537, 0.0542]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2415, device='cuda:0', dtype=torch.float64)\n",
      "done with validation\n",
      "torch.Size([162])\n",
      "[1] Training Accuracy: 0.000 Validation Accuracy: 0.000\n",
      "Train Loss: 4.6106 Validation Loss: 4.7191\n",
      "Best Validation Accuracy: 0.000\n",
      "Run time for 1 epochs was 11.72 minutes (0.20 hours)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "number_of_leafs = len(leaf_values)\n",
    "number_of_parents = len(internal_values)\n",
    "\n",
    "print('There are', number_of_leafs, 'leafs and', number_of_parents, 'parents.')\n",
    "\n",
    "print('Number of training batches is {:.2f}'.format(experiment_datapipe.shape[0]*train_percent/batch_size))\n",
    "print('Number of validation batches is {:.2f}'.format(experiment_datapipe.shape[0]*val_percent/batch_size))\n",
    "\n",
    "# create dataframe that only includes leaf nodes\n",
    "ontology_leaf_df = ontology_df[leaf_values]\n",
    "\n",
    "# Create dictionary to map between two different types of encoded values\n",
    "# get the encoder from the datapipe\n",
    "cell_type_encoder = experiment_datapipe.obs_encoders[\"cell_type_ontology_term_id\"]\n",
    "\n",
    "# build the dictionary of encoded values from the datapipe\n",
    "encoder_mapping_dict = dict(zip(cell_type_encoder.classes_,cell_type_encoder.transform(cell_type_encoder.classes_)))\n",
    "\n",
    "# build the dictionary mapping from encoder_mapping_dict (keys) to mapping_dict (values)\n",
    "encoding_mapper = {}\n",
    "for cell_term in encoder_mapping_dict.keys():\n",
    "    encoding_mapper[encoder_mapping_dict[cell_term]] = mapping_dict[cell_term]\n",
    "\n",
    "\n",
    "# set the prediction threshold for internal nodes\n",
    "threshold = 0.1 #0.8\n",
    "print('Prediction threshold for internal nodes is', threshold)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "marginalization_dict = marginalization_classification_manual_batch(train_dataloader,val_dataloader,\n",
    "                                                                   num_epochs, ontology_leaf_df, batch_size,\n",
    "                                                                  internal_values,mapping_dict,\n",
    "                                                                  ontology_df,threshold, cell_parent_mask,encoding_mapper)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_minutes = (end-start)/60.\n",
    "time_hours = (end-start)/3600.\n",
    "\n",
    "print(f'Run time for {num_epochs} epochs was {time_minutes:.2f} minutes ({time_hours:.2f} hours)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb3866",
   "metadata": {},
   "source": [
    "#### Testing running times per batch\n",
    "Note that I'm only running a handful of batches, so this probbably does not account for any extra time needed to load the next batch of data from disk. That would require a longer run, so soma_chunck_size might not really matter here. And there is some variability in how long each batch takes to run. I'm not sure if this is related to disk streaming or not. This is for the full set of 2.7 million cells\n",
    "\n",
    "|batch size | soma chunk size| time per batch (s) | # of batches | time per 1 training epoch (hr) |\n",
    "| -------- | ------- |------ | ----- | ----- |\n",
    "|8192 | 10,000 | 190 | 266 | 14\n",
    "| 32768 | 10,000 | 600 | 66 | 11\n",
    "| 8192 | 20,000 | 180 | 266 | 14\n",
    "| 4096 | 20,000 | 45 | 532 | 6.7\n",
    "| 1024 | 20,000 | 15 | 2129 | 8.8\n",
    "| 512 | 20,000 |  4.5 | 4259  | 5.3\n",
    "| 256 | 20,000 | 1.4  | 8518  | 3.3\n",
    "| 256 | 30,000 |  1.8 | 8518   | 4.4\n",
    "| 128 | 30,000 | 1.06  | 17037  | 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ca2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9a904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36cf22a5",
   "metadata": {},
   "source": [
    "## Save portions of the modeling and visualize results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd4945eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get today's date for saving information about this model\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model parameters to a file\n",
    "\n",
    "#marginalization_dict['best_state_dict']\n",
    "\n",
    "model_title = today + '_best_model'\n",
    "\n",
    "torch.save(marginalization_dict['best_state_dict'],model_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "326a62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the loss/accuracy/f1 scores to a file\n",
    "\n",
    "cols_to_save = ['accuracy_train_leaf_hist','loss_train_hist', \n",
    "                'loss_train_leaf_hist', 'loss_train_internal_hist', \n",
    "                'accuracy_val_leaf_hist', \n",
    "                'loss_val_hist', 'loss_val_leaf_hist', 'loss_val_internal_hist', \n",
    "                'f1_score_train_leaf', 'f1_score_val_leaf',\n",
    "               'f1_score_train_internal','f1_score_val_internal']\n",
    "\n",
    "results_dict = {key: marginalization_dict[key] for key in cols_to_save}\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_dict, orient='columns')\n",
    "\n",
    "results_df.insert(0,'epoch',range(1,num_epochs+1))\n",
    "\n",
    "results_title = today + '_results.csv'\n",
    "\n",
    "\n",
    "results_df.to_csv(results_title,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70b0081b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginalization_dict['loss_val_hist']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c32fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5b9afce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/cell_classification/env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/cell_classification/env/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/cell_classification/env/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m plot_title \u001b[38;5;241m=\u001b[39m today \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_results.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# if you want to save the plot\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mplot_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarginalization_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_title\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# just display the plot\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#plot_results(marginalization_dict,num_epochs)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mplot_results\u001b[0;34m(marginalization_dict, num_epochs, save_title)\u001b[0m\n\u001b[1;32m     12\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),y\u001b[38;5;241m=\u001b[39mmarginalization_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy_val_internal_hist\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     13\u001b[0m                ax \u001b[38;5;241m=\u001b[39m ax[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m],color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmediumslateblue\u001b[39m\u001b[38;5;124m'\u001b[39m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),y\u001b[38;5;241m=\u001b[39mmarginalization_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_train_hist\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     16\u001b[0m                 ax \u001b[38;5;241m=\u001b[39m ax[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m],color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightcoral\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlineplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarginalization_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss_val_hist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmediumslateblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),y\u001b[38;5;241m=\u001b[39mmarginalization_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_train_leaf_hist\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     21\u001b[0m                 ax \u001b[38;5;241m=\u001b[39m ax[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m],color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightcoral\u001b[39m\u001b[38;5;124m'\u001b[39m,marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Leafs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),y\u001b[38;5;241m=\u001b[39mmarginalization_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_train_internal_hist\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     23\u001b[0m                 ax \u001b[38;5;241m=\u001b[39m ax[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m],color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightcoral\u001b[39m\u001b[38;5;124m'\u001b[39m,marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Internal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/cell_classification/env/lib/python3.10/site-packages/seaborn/relational.py:645\u001b[0m, in \u001b[0;36mlineplot\u001b[0;34m(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m color \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    643\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _default_color(ax\u001b[38;5;241m.\u001b[39mplot, hue, color, kwargs)\n\u001b[0;32m--> 645\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ax\n",
      "File \u001b[0;32m~/cell_classification/env/lib/python3.10/site-packages/seaborn/relational.py:459\u001b[0m, in \u001b[0;36m_LinePlotter.plot\u001b[0;34m(self, ax, kws)\u001b[0m\n\u001b[1;32m    457\u001b[0m         lines\u001b[38;5;241m.\u001b[39mextend(ax\u001b[38;5;241m.\u001b[39mplot(unit_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m], unit_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkws))\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     lines \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mplot(sub_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43msub_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkws)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sub_vars:\n",
      "File \u001b[0;32m~/cell_classification/env/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/cell_classification/env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'y'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x1200 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_title = today + '_results.png'\n",
    "\n",
    "# if you want to save the plot\n",
    "plot_results(marginalization_dict,num_epochs,save_title=plot_title)\n",
    "\n",
    "# just display the plot\n",
    "#plot_results(marginalization_dict,num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb849b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4dcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c82cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1592e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c21039bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CL:0000037': 0,\n",
       " 'CL:0000038': 1,\n",
       " 'CL:0000049': 2,\n",
       " 'CL:0000050': 3,\n",
       " 'CL:0000051': 4,\n",
       " 'CL:0000081': 5,\n",
       " 'CL:0000084': 6,\n",
       " 'CL:0000091': 7,\n",
       " 'CL:0000092': 8,\n",
       " 'CL:0000094': 9,\n",
       " 'CL:0000097': 10,\n",
       " 'CL:0000129': 11,\n",
       " 'CL:0000232': 12,\n",
       " 'CL:0000233': 13,\n",
       " 'CL:0000235': 14,\n",
       " 'CL:0000236': 15,\n",
       " 'CL:0000451': 16,\n",
       " 'CL:0000453': 17,\n",
       " 'CL:0000492': 18,\n",
       " 'CL:0000542': 19,\n",
       " 'CL:0000547': 20,\n",
       " 'CL:0000556': 21,\n",
       " 'CL:0000557': 22,\n",
       " 'CL:0000559': 23,\n",
       " 'CL:0000576': 24,\n",
       " 'CL:0000583': 25,\n",
       " 'CL:0000595': 26,\n",
       " 'CL:0000623': 27,\n",
       " 'CL:0000624': 28,\n",
       " 'CL:0000625': 29,\n",
       " 'CL:0000738': 30,\n",
       " 'CL:0000763': 31,\n",
       " 'CL:0000764': 32,\n",
       " 'CL:0000765': 33,\n",
       " 'CL:0000766': 34,\n",
       " 'CL:0000767': 35,\n",
       " 'CL:0000771': 36,\n",
       " 'CL:0000775': 37,\n",
       " 'CL:0000776': 38,\n",
       " 'CL:0000782': 39,\n",
       " 'CL:0000784': 40,\n",
       " 'CL:0000785': 41,\n",
       " 'CL:0000786': 42,\n",
       " 'CL:0000787': 43,\n",
       " 'CL:0000788': 44,\n",
       " 'CL:0000789': 45,\n",
       " 'CL:0000791': 46,\n",
       " 'CL:0000794': 47,\n",
       " 'CL:0000798': 48,\n",
       " 'CL:0000800': 49,\n",
       " 'CL:0000807': 50,\n",
       " 'CL:0000808': 51,\n",
       " 'CL:0000809': 52,\n",
       " 'CL:0000810': 53,\n",
       " 'CL:0000811': 54,\n",
       " 'CL:0000813': 55,\n",
       " 'CL:0000814': 56,\n",
       " 'CL:0000815': 57,\n",
       " 'CL:0000816': 58,\n",
       " 'CL:0000817': 59,\n",
       " 'CL:0000818': 60,\n",
       " 'CL:0000823': 61,\n",
       " 'CL:0000826': 62,\n",
       " 'CL:0000836': 63,\n",
       " 'CL:0000837': 64,\n",
       " 'CL:0000838': 65,\n",
       " 'CL:0000839': 66,\n",
       " 'CL:0000841': 67,\n",
       " 'CL:0000844': 68,\n",
       " 'CL:0000860': 69,\n",
       " 'CL:0000861': 70,\n",
       " 'CL:0000863': 71,\n",
       " 'CL:0000875': 72,\n",
       " 'CL:0000878': 73,\n",
       " 'CL:0000890': 74,\n",
       " 'CL:0000893': 75,\n",
       " 'CL:0000894': 76,\n",
       " 'CL:0000895': 77,\n",
       " 'CL:0000896': 78,\n",
       " 'CL:0000897': 79,\n",
       " 'CL:0000898': 80,\n",
       " 'CL:0000899': 81,\n",
       " 'CL:0000900': 82,\n",
       " 'CL:0000903': 83,\n",
       " 'CL:0000904': 84,\n",
       " 'CL:0000905': 85,\n",
       " 'CL:0000906': 86,\n",
       " 'CL:0000907': 87,\n",
       " 'CL:0000908': 88,\n",
       " 'CL:0000909': 89,\n",
       " 'CL:0000910': 90,\n",
       " 'CL:0000912': 91,\n",
       " 'CL:0000913': 92,\n",
       " 'CL:0000915': 93,\n",
       " 'CL:0000921': 94,\n",
       " 'CL:0000934': 95,\n",
       " 'CL:0000936': 96,\n",
       " 'CL:0000938': 97,\n",
       " 'CL:0000939': 98,\n",
       " 'CL:0000940': 99,\n",
       " 'CL:0000970': 100,\n",
       " 'CL:0000972': 101,\n",
       " 'CL:0000979': 102,\n",
       " 'CL:0000980': 103,\n",
       " 'CL:0000985': 104,\n",
       " 'CL:0000987': 105,\n",
       " 'CL:0000990': 106,\n",
       " 'CL:0001029': 107,\n",
       " 'CL:0001043': 108,\n",
       " 'CL:0001044': 109,\n",
       " 'CL:0001049': 110,\n",
       " 'CL:0001050': 111,\n",
       " 'CL:0001054': 112,\n",
       " 'CL:0001056': 113,\n",
       " 'CL:0001057': 114,\n",
       " 'CL:0001058': 115,\n",
       " 'CL:0001062': 116,\n",
       " 'CL:0001065': 117,\n",
       " 'CL:0001071': 118,\n",
       " 'CL:0001078': 119,\n",
       " 'CL:0001082': 120,\n",
       " 'CL:0001203': 121,\n",
       " 'CL:0002038': 122,\n",
       " 'CL:0002045': 123,\n",
       " 'CL:0002046': 124,\n",
       " 'CL:0002057': 125,\n",
       " 'CL:0002117': 126,\n",
       " 'CL:0002343': 127,\n",
       " 'CL:0002355': 128,\n",
       " 'CL:0002393': 129,\n",
       " 'CL:0002394': 130,\n",
       " 'CL:0002396': 131,\n",
       " 'CL:0002397': 132,\n",
       " 'CL:0002399': 133,\n",
       " 'CL:0002419': 134,\n",
       " 'CL:0002425': 135,\n",
       " 'CL:0002489': 136,\n",
       " 'CL:0002496': 137,\n",
       " 'CL:0002677': 138,\n",
       " 'CL:0002678': 139,\n",
       " 'CL:1001603': 140,\n",
       " 'CL:2000055': 141,\n",
       " 'CL:3000001': 142}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "687a4ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CL:0000763': -9999,\n",
       " 'CL:0000542': -9998,\n",
       " 'CL:0000097': 0,\n",
       " 'CL:0002046': 1,\n",
       " 'CL:0000817': 2,\n",
       " 'CL:0000051': 3,\n",
       " 'CL:0000826': -9997,\n",
       " 'CL:0001029': 4,\n",
       " 'CL:0000990': -9996,\n",
       " 'CL:0000785': -9995,\n",
       " 'CL:0000816': 5,\n",
       " 'CL:0000786': -9994,\n",
       " 'CL:0000784': -9993,\n",
       " 'CL:0000557': 6,\n",
       " 'CL:0000084': -9992,\n",
       " 'CL:0000814': -9991,\n",
       " 'CL:0000837': -9990,\n",
       " 'CL:0000037': 7,\n",
       " 'CL:0000576': -9989,\n",
       " 'CL:0000050': 8,\n",
       " 'CL:0000129': 9,\n",
       " 'CL:0000815': -9988,\n",
       " 'CL:0000912': 10,\n",
       " 'CL:0000940': 11,\n",
       " 'CL:0000623': -9987,\n",
       " 'CL:0000899': 12,\n",
       " 'CL:0000798': -9986,\n",
       " 'CL:0000235': -9985,\n",
       " 'CL:0000775': -9984,\n",
       " 'CL:0000236': -9983,\n",
       " 'CL:0000453': 13,\n",
       " 'CL:0002343': 14,\n",
       " 'CL:0001078': 15,\n",
       " 'CL:3000001': 16,\n",
       " 'CL:0000451': -9982,\n",
       " 'CL:0000094': -9981,\n",
       " 'CL:0000738': -9980,\n",
       " 'CL:0000878': -9979,\n",
       " 'CL:0000972': -9978,\n",
       " 'CL:0000788': 17,\n",
       " 'CL:0000985': 18,\n",
       " 'CL:0000987': 19,\n",
       " 'CL:0000970': 20,\n",
       " 'CL:0000913': 21,\n",
       " 'CL:0000492': -9977,\n",
       " 'CL:0000624': -9976,\n",
       " 'CL:0000909': -9975,\n",
       " 'CL:0000896': -9974,\n",
       " 'CL:0000906': -9973,\n",
       " 'CL:0000905': 22,\n",
       " 'CL:0000890': 23,\n",
       " 'CL:0000860': -9972,\n",
       " 'CL:0000875': -9971,\n",
       " 'CL:0000782': -9970,\n",
       " 'CL:0000863': 24,\n",
       " 'CL:0000767': 25,\n",
       " 'CL:0000583': 26,\n",
       " 'CL:0000625': -9969,\n",
       " 'CL:0000861': -9968,\n",
       " 'CL:1001603': -9967,\n",
       " 'CL:0002399': 27,\n",
       " 'CL:0001071': -9966,\n",
       " 'CL:0000766': -9965,\n",
       " 'CL:0000765': 28,\n",
       " 'CL:0002496': -9964,\n",
       " 'CL:0000898': -9963,\n",
       " 'CL:0001065': -9962,\n",
       " 'CL:0000903': 29,\n",
       " 'CL:0000787': -9961,\n",
       " 'CL:0000813': -9960,\n",
       " 'CL:0000232': -9959,\n",
       " 'CL:0000910': 30,\n",
       " 'CL:0000838': -9958,\n",
       " 'CL:0000839': -9957,\n",
       " 'CL:0000038': 31,\n",
       " 'CL:0000547': 32,\n",
       " 'CL:0002355': 33,\n",
       " 'CL:0002045': 34,\n",
       " 'CL:0000559': 35,\n",
       " 'CL:0001054': -9956,\n",
       " 'CL:0000836': 36,\n",
       " 'CL:0000556': 37,\n",
       " 'CL:0000092': 38,\n",
       " 'CL:0000938': 39,\n",
       " 'CL:0000936': 40,\n",
       " 'CL:0000049': 41,\n",
       " 'CL:0000771': 42,\n",
       " 'CL:0002419': -9955,\n",
       " 'CL:0001082': -9954,\n",
       " 'CL:0002489': -9953,\n",
       " 'CL:0000809': 43,\n",
       " 'CL:0002425': 44,\n",
       " 'CL:0000915': 45,\n",
       " 'CL:0000897': -9952,\n",
       " 'CL:0000789': -9951,\n",
       " 'CL:0000904': 46,\n",
       " 'CL:0000900': 47,\n",
       " 'CL:0002396': 48,\n",
       " 'CL:0000895': 49,\n",
       " 'CL:0000934': 50,\n",
       " 'CL:0000907': 51,\n",
       " 'CL:0002677': 52,\n",
       " 'CL:0000980': 53,\n",
       " 'CL:0002678': 54,\n",
       " 'CL:0000233': 55,\n",
       " 'CL:0001057': 56,\n",
       " 'CL:0000091': 57,\n",
       " 'CL:0000939': 58,\n",
       " 'CL:0001203': -9950,\n",
       " 'CL:0002038': 59,\n",
       " 'CL:0000764': -9949,\n",
       " 'CL:0001062': 60,\n",
       " 'CL:0001056': -9948,\n",
       " 'CL:0000844': 61,\n",
       " 'CL:0001044': 62,\n",
       " 'CL:0000979': 63,\n",
       " 'CL:0001050': 64,\n",
       " 'CL:0002117': 65,\n",
       " 'CL:0002393': 66,\n",
       " 'CL:0000081': -9947,\n",
       " 'CL:0000595': 67,\n",
       " 'CL:0000810': 68,\n",
       " 'CL:0000811': 69,\n",
       " 'CL:0002394': 70,\n",
       " 'CL:2000055': 71,\n",
       " 'CL:0000908': 72,\n",
       " 'CL:0000921': 73,\n",
       " 'CL:0000841': 74,\n",
       " 'CL:0000794': 75,\n",
       " 'CL:0000807': 76,\n",
       " 'CL:0000894': 77,\n",
       " 'CL:0000808': 78,\n",
       " 'CL:0000823': 79,\n",
       " 'CL:0000893': -9946,\n",
       " 'CL:0002057': 80,\n",
       " 'CL:0000791': -9945,\n",
       " 'CL:0002397': 81,\n",
       " 'CL:0000800': 82,\n",
       " 'CL:0001058': 83,\n",
       " 'CL:0000818': 84,\n",
       " 'CL:0001043': 85,\n",
       " 'CL:0001049': 86,\n",
       " 'CL:0000776': 87}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d98802bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be27cda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-9999: 31,\n",
       " -9998: 19,\n",
       " 0: 10,\n",
       " 1: 124,\n",
       " 2: 59,\n",
       " 3: 4,\n",
       " -9997: 62,\n",
       " 4: 107,\n",
       " -9996: 106,\n",
       " -9995: 41,\n",
       " 5: 58,\n",
       " -9994: 42,\n",
       " -9993: 40,\n",
       " 6: 22,\n",
       " -9992: 6,\n",
       " -9991: 56,\n",
       " -9990: 64,\n",
       " 7: 0,\n",
       " -9989: 24,\n",
       " 8: 3,\n",
       " 9: 11,\n",
       " -9988: 57,\n",
       " 10: 91,\n",
       " 11: 99,\n",
       " -9987: 27,\n",
       " 12: 81,\n",
       " -9986: 48,\n",
       " -9985: 14,\n",
       " -9984: 37,\n",
       " -9983: 15,\n",
       " 13: 17,\n",
       " 14: 127,\n",
       " 15: 119,\n",
       " 16: 142,\n",
       " -9982: 16,\n",
       " -9981: 9,\n",
       " -9980: 30,\n",
       " -9979: 73,\n",
       " -9978: 101,\n",
       " 17: 44,\n",
       " 18: 104,\n",
       " 19: 105,\n",
       " 20: 100,\n",
       " 21: 92,\n",
       " -9977: 18,\n",
       " -9976: 28,\n",
       " -9975: 89,\n",
       " -9974: 78,\n",
       " -9973: 86,\n",
       " 22: 85,\n",
       " 23: 74,\n",
       " -9972: 69,\n",
       " -9971: 72,\n",
       " -9970: 39,\n",
       " 24: 71,\n",
       " 25: 35,\n",
       " 26: 25,\n",
       " -9969: 29,\n",
       " -9968: 70,\n",
       " -9967: 140,\n",
       " 27: 133,\n",
       " -9966: 118,\n",
       " -9965: 34,\n",
       " 28: 33,\n",
       " -9964: 137,\n",
       " -9963: 80,\n",
       " -9962: 117,\n",
       " 29: 83,\n",
       " -9961: 43,\n",
       " -9960: 55,\n",
       " -9959: 12,\n",
       " 30: 90,\n",
       " -9958: 65,\n",
       " -9957: 66,\n",
       " 31: 1,\n",
       " 32: 20,\n",
       " 33: 128,\n",
       " 34: 123,\n",
       " 35: 23,\n",
       " -9956: 112,\n",
       " 36: 63,\n",
       " 37: 21,\n",
       " 38: 8,\n",
       " 39: 97,\n",
       " 40: 96,\n",
       " 41: 2,\n",
       " 42: 36,\n",
       " -9955: 134,\n",
       " -9954: 120,\n",
       " -9953: 136,\n",
       " 43: 52,\n",
       " 44: 135,\n",
       " 45: 93,\n",
       " -9952: 79,\n",
       " -9951: 45,\n",
       " 46: 84,\n",
       " 47: 82,\n",
       " 48: 131,\n",
       " 49: 77,\n",
       " 50: 95,\n",
       " 51: 87,\n",
       " 52: 138,\n",
       " 53: 103,\n",
       " 54: 139,\n",
       " 55: 13,\n",
       " 56: 114,\n",
       " 57: 7,\n",
       " 58: 98,\n",
       " -9950: 121,\n",
       " 59: 122,\n",
       " -9949: 32,\n",
       " 60: 116,\n",
       " -9948: 113,\n",
       " 61: 68,\n",
       " 62: 109,\n",
       " 63: 102,\n",
       " 64: 111,\n",
       " 65: 126,\n",
       " 66: 129,\n",
       " -9947: 5,\n",
       " 67: 26,\n",
       " 68: 53,\n",
       " 69: 54,\n",
       " 70: 130,\n",
       " 71: 141,\n",
       " 72: 88,\n",
       " 73: 94,\n",
       " 74: 67,\n",
       " 75: 47,\n",
       " 76: 50,\n",
       " 77: 76,\n",
       " 78: 51,\n",
       " 79: 61,\n",
       " -9946: 75,\n",
       " 80: 125,\n",
       " -9945: 46,\n",
       " 81: 132,\n",
       " 82: 49,\n",
       " 83: 115,\n",
       " 84: 60,\n",
       " 85: 108,\n",
       " 86: 110,\n",
       " 87: 38}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd2142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d75f938d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CL:0000037', 'CL:0000038', 'CL:0000049', 'CL:0000050',\n",
       "       'CL:0000051', 'CL:0000081', 'CL:0000084', 'CL:0000091',\n",
       "       'CL:0000092', 'CL:0000094', 'CL:0000097', 'CL:0000129',\n",
       "       'CL:0000232', 'CL:0000233', 'CL:0000235', 'CL:0000236',\n",
       "       'CL:0000451', 'CL:0000453', 'CL:0000492', 'CL:0000542',\n",
       "       'CL:0000547', 'CL:0000556', 'CL:0000557', 'CL:0000559',\n",
       "       'CL:0000576', 'CL:0000583', 'CL:0000595', 'CL:0000623',\n",
       "       'CL:0000624', 'CL:0000625', 'CL:0000738', 'CL:0000763',\n",
       "       'CL:0000764', 'CL:0000765', 'CL:0000766', 'CL:0000767',\n",
       "       'CL:0000771', 'CL:0000775', 'CL:0000776', 'CL:0000782',\n",
       "       'CL:0000784', 'CL:0000785', 'CL:0000786', 'CL:0000787',\n",
       "       'CL:0000788', 'CL:0000789', 'CL:0000791', 'CL:0000794',\n",
       "       'CL:0000798', 'CL:0000800', 'CL:0000807', 'CL:0000808',\n",
       "       'CL:0000809', 'CL:0000810', 'CL:0000811', 'CL:0000813',\n",
       "       'CL:0000814', 'CL:0000815', 'CL:0000816', 'CL:0000817',\n",
       "       'CL:0000818', 'CL:0000823', 'CL:0000826', 'CL:0000836',\n",
       "       'CL:0000837', 'CL:0000838', 'CL:0000839', 'CL:0000841',\n",
       "       'CL:0000844', 'CL:0000860', 'CL:0000861', 'CL:0000863',\n",
       "       'CL:0000875', 'CL:0000878', 'CL:0000890', 'CL:0000893',\n",
       "       'CL:0000894', 'CL:0000895', 'CL:0000896', 'CL:0000897',\n",
       "       'CL:0000898', 'CL:0000899', 'CL:0000900', 'CL:0000903',\n",
       "       'CL:0000904', 'CL:0000905', 'CL:0000906', 'CL:0000907',\n",
       "       'CL:0000908', 'CL:0000909', 'CL:0000910', 'CL:0000912',\n",
       "       'CL:0000913', 'CL:0000915', 'CL:0000921', 'CL:0000934',\n",
       "       'CL:0000936', 'CL:0000938', 'CL:0000939', 'CL:0000940',\n",
       "       'CL:0000970', 'CL:0000972', 'CL:0000979', 'CL:0000980',\n",
       "       'CL:0000985', 'CL:0000987', 'CL:0000990', 'CL:0001029',\n",
       "       'CL:0001043', 'CL:0001044', 'CL:0001049', 'CL:0001050',\n",
       "       'CL:0001054', 'CL:0001056', 'CL:0001057', 'CL:0001058',\n",
       "       'CL:0001062', 'CL:0001065', 'CL:0001071', 'CL:0001078',\n",
       "       'CL:0001082', 'CL:0001203', 'CL:0002038', 'CL:0002045',\n",
       "       'CL:0002046', 'CL:0002057', 'CL:0002117', 'CL:0002343',\n",
       "       'CL:0002355', 'CL:0002393', 'CL:0002394', 'CL:0002396',\n",
       "       'CL:0002397', 'CL:0002399', 'CL:0002419', 'CL:0002425',\n",
       "       'CL:0002489', 'CL:0002496', 'CL:0002677', 'CL:0002678',\n",
       "       'CL:1001603', 'CL:2000055', 'CL:3000001'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_type_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13d65d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LabelEncoder in module sklearn.preprocessing._label object:\n",
      "\n",
      "class LabelEncoder(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  Encode target labels with value between 0 and n_classes-1.\n",
      " |  \n",
      " |  This transformer should be used to encode target values, *i.e.* `y`, and\n",
      " |  not the input `X`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_targets>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.12\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      Holds the label for each class.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  OrdinalEncoder : Encode categorical features using an ordinal encoding\n",
      " |      scheme.\n",
      " |  OneHotEncoder : Encode categorical features as a one-hot numeric array.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  `LabelEncoder` can be used to normalize labels.\n",
      " |  \n",
      " |  >>> from sklearn import preprocessing\n",
      " |  >>> le = preprocessing.LabelEncoder()\n",
      " |  >>> le.fit([1, 2, 2, 6])\n",
      " |  LabelEncoder()\n",
      " |  >>> le.classes_\n",
      " |  array([1, 2, 6])\n",
      " |  >>> le.transform([1, 1, 2, 6])\n",
      " |  array([0, 0, 1, 2]...)\n",
      " |  >>> le.inverse_transform([0, 0, 1, 2])\n",
      " |  array([1, 1, 2, 6])\n",
      " |  \n",
      " |  It can also be used to transform non-numerical labels (as long as they are\n",
      " |  hashable and comparable) to numerical labels.\n",
      " |  \n",
      " |  >>> le = preprocessing.LabelEncoder()\n",
      " |  >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
      " |  LabelEncoder()\n",
      " |  >>> list(le.classes_)\n",
      " |  ['amsterdam', 'paris', 'tokyo']\n",
      " |  >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
      " |  array([2, 2, 1]...)\n",
      " |  >>> list(le.inverse_transform([2, 2, 1]))\n",
      " |  ['tokyo', 'tokyo', 'paris']\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LabelEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, y)\n",
      " |      Fit label encoder.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |          Fitted label encoder.\n",
      " |  \n",
      " |  fit_transform(self, y)\n",
      " |      Fit label encoder and return encoded labels.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Encoded labels.\n",
      " |  \n",
      " |  inverse_transform(self, y)\n",
      " |      Transform labels back to original encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Original encoding.\n",
      " |  \n",
      " |  transform(self, y)\n",
      " |      Transform labels to normalized encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Labels as normalized encodings.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set output container.\n",
      " |      \n",
      " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      " |      for an example on how to use the API.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |      \n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cell_type_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47897d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f58d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1860a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
