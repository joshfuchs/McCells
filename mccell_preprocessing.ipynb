{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc9153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import umap\n",
    "\n",
    "import pronto\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pronto.warnings.ProntoWarning)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torcheval.metrics.functional import multilabel_accuracy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(style='whitegrid')\n",
    "sns.set_context(context='notebook')\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc(\n",
    "    'axes',\n",
    "    labelweight='bold',\n",
    "    labelsize='large',\n",
    "    titleweight='bold',\n",
    "    titlesize=9,\n",
    "    linewidth=4\n",
    "    )\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc04366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a6b16",
   "metadata": {},
   "source": [
    "## Load the data and Cell Ontology. \n",
    "\n",
    "Great-lakes does not have internet access, so we pull in the data outside first, then load it here as an AnnData object. The Cell Ontology also needs to be loaded for access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b732a6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nfs/turbo/umms-welchjd/mccell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# change into scratch directory (where data is saved)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#os.chdir('/scratch/welchjd_root/welchjd99/fujoshua')\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/nfs/turbo/umms-welchjd/mccell\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nfs/turbo/umms-welchjd/mccell'"
     ]
    }
   ],
   "source": [
    "# change into scratch directory (where data is saved)\n",
    "#os.chdir('/scratch/welchjd_root/welchjd99/fujoshua')\n",
    "os.chdir('/nfs/turbo/umms-welchjd/mccell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab383ef",
   "metadata": {},
   "source": [
    "### Load Single Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a84502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adata = ad.read_h5ad('small_1044_900') # 42000 cells\n",
    "#adata = ad.read_h5ad('leaf_list_in_cl') # 658000 cells\n",
    "#adata = ad.read_h5ad('leaf_list_leukocyte_24Aug') # 516000 cells, 60k genes\n",
    "#adata = ad.read_h5ad('leaf_list_leukocyte_6Sep_coding_genes') # 516k cells, 20k genes\n",
    "#adata = ad.read_h5ad('leaf_list_hematopoietic_14Sep_coding_genes') # 549k cells, 20k genes, upper level hematopoietic (0000988)\n",
    "#adata = ad.read_h5ad('1044_624_895_27Sep_coding_genes') # 230k cells, only CL1044 (leaf), 624 (internal), and 895 (leaf) including leaf and internal node\n",
    "#adata = ad.read_h5ad('13Oct_2int_3leaf') #379k cells, 2 internal (576,624), 3 leaf (1044,895,2057)\n",
    "\n",
    "#adata = ad.read_h5ad('24Oct_hematopoietic_cells_p2') # 472k cells, part of the full hema data set\n",
    "adata = ad.read_h5ad('24Oct_hematopoietic_cells_p2_subsample') # 47k cells, part of the full hema data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4286bb5",
   "metadata": {},
   "source": [
    "### Load Multiple Datasets and combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata1 = ad.read_h5ad('24Oct_hematopoietic_cells_p1') # 1.98 million cells\n",
    "adata2 = ad.read_h5ad('24Oct_hematopoietic_cells_p2') # 472k cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.concat([adata1,adata2]) #2.45 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23051f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete adata1 and adata2 to save memory\n",
    "del adata1\n",
    "del adata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b157d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb09904",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dd9ebe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/josh.fuchs/My Drive/Personal/michigan'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9b85a",
   "metadata": {},
   "source": [
    "### Load the Cell Ontology\n",
    "\n",
    "You can visualize the ontology using https://www.ebi.ac.uk/ols4/ontologies/cl\n",
    "\n",
    "And you can download the ontology file here: https://obofoundry.org/ontology/cl.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f23ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/fujoshua/cell_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06e0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = pronto.Ontology.from_obo_library('cl.owl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6fdf4",
   "metadata": {},
   "source": [
    "## Checking memory allocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_properties(0).total_memory*1e-9)\n",
    "print(torch.cuda.memory_reserved()*1e-9)\n",
    "print(torch.cuda.memory_allocated()*1e-9)\n",
    "#print(torch.cuda.mem_get_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13767347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c966afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         adata: 806.6 MiB\n",
      "                  LabelEncoder:  1.0 KiB\n",
      "                           _i1:  966.0 B\n",
      "                           _i4:  913.0 B\n",
      "                           _i5:  913.0 B\n",
      "                      datetime:  416.0 B\n",
      "                            _i:  371.0 B\n",
      "                           _i8:  371.0 B\n",
      "                           _i9:  260.0 B\n",
      "                           _oh:  232.0 B\n"
     ]
    }
   ],
   "source": [
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n",
    "                          locals().items())), key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "del adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43e876",
   "metadata": {},
   "source": [
    "## Data and Ontology Preprocessing\n",
    "\n",
    "To prepare the data for modeling, we need to perform some preprocessing on the data and the ontology. We'll use three functions to make this happen. Full descriptions of these functions can be found in the functions. \n",
    "\n",
    "\n",
    "- set_internal_node_values: build a dictionary to set which internal nodes are to be used in the loss calculation for internal nodes in the data\n",
    "\n",
    "- build_parent_mask: builds a masking matrix to use for masking internal node loss values\n",
    "\n",
    "- preprocess_data_ontology: this function encodes the AnnData object, splits apart the target values and primary data, and calculates some important variables from the Cell Ontology for later use \n",
    "\n",
    "- transform_data: transforms the data with log(1+x)\n",
    "\n",
    "- split_format_data: splits the data into train and validation sets, and moves the variables to PyTorch tensors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a58cec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_internal_node_values(internal_values,all_parent_nodes):\n",
    "    '''\n",
    "    Creates a dictionary where each key is an internal cell type and the values are the cell types\n",
    "    we want to include when calculating the loss. We do not want to consider direct descendents of the\n",
    "    internal cell type, so those are removed. \n",
    "    \n",
    "    In other words, when calculating the loss for an internal node, we want to include all internal \n",
    "    nodes in the ontology EXCEPT those that are direct descendants of the target internal node. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    internal_values : list\n",
    "        list of internal values that are included in the dataset\n",
    "            \n",
    "    all_parent_nodes : list\n",
    "        from the dataset, a list of parent nodes in the ontology. Used to remove portions of\n",
    "        the Ontology where we do not have child data\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    parent_dict : dictionary\n",
    "        keys are internal_values and values are all internal cell ontology terms EXCEPT descendents \n",
    "        of the internal value. The internal value is always included\n",
    "    '''\n",
    "    \n",
    "    parent_dict = {}\n",
    "\n",
    "    # loop through each value to calculate the values to include in parent_dict for that\n",
    "    # internal value\n",
    "    for internal_node in internal_values:\n",
    "        # 1) get the children of this internal_node\n",
    "        child_nodes = []\n",
    "        for term in cl[internal_node].subclasses(distance=None,with_self=False).to_set():\n",
    "            child_nodes.append(term.id)\n",
    "        \n",
    "        # 2) remove those values from all_parent_nodes\n",
    "        cell_types_to_include = [x for x in all_parent_nodes if x not in child_nodes]\n",
    "        \n",
    "        # 3) create dictionary\n",
    "        parent_dict[internal_node] = cell_types_to_include\n",
    "    \n",
    "    return(parent_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3f42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parent_mask(leaf_values,internal_values,ontology_df,parent_dict):\n",
    "    '''\n",
    "    Function to build a masking matrix for use when calculating the internal loss\n",
    "    \n",
    "    Uses parent_dict to denote, for internal cell types, which parents to include in the loss\n",
    "    calculation. \n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    leaf_values : list\n",
    "        list composed of all leaf values included in the dataset\n",
    "        includes internal nodes that do not have sub-values in the dataset, and thus are\n",
    "        treated an leaf nodes\n",
    "\n",
    "    internal_values : list\n",
    "        list composed of interanal nodes in the dataset\n",
    "\n",
    "    ontology_df : pandas dataframe\n",
    "        pandas dataframe where indices (rows) are all leaf and parent cell IDs from the portion of \n",
    "        the ontology being queried, and columns are all leafs in portion of ontology being queried. \n",
    "        \n",
    "        Dataframe is binary. For each parent node, element = 1 if parent node is an ancestor\n",
    "        of corresponding leaf node.\n",
    "        \n",
    "    parent_dict : dictionary\n",
    "        keys are internal_values and values are all internal cell ontology terms EXCEPT descendents \n",
    "        of the internal value. The internal value is always included\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cell_parents_mask : tensor\n",
    "        tensor of shape ik, where i = parent IDs and k = each cell type in the dataset\n",
    "        binary tensor where 1 means for that cell type, that parent ID will be included\n",
    "        in the internal loss calculation\n",
    "        and 0 means for that cell type, that parent ID is excluded in the internal loss\n",
    "        calculation\n",
    "    \n",
    "    '''\n",
    "    num_leafs = len(leaf_values)\n",
    "    num_parents = ontology_df.shape[0]\n",
    "\n",
    "    # internal_values are included as column values AND rows\n",
    "\n",
    "\n",
    "    # for cell_parents_to_include, each column is a cell type included in the\n",
    "    # dataset, so it is length = len(leaf_values) + len(internal_values)\n",
    "    # the row values are the total number of parents included for the dataset \n",
    "    # for each internal value, we need to pick (1/0) if we include that parent\n",
    "    # for the loss. For this, we reference parents_dict\n",
    "    # WHAT is the order of the cell IDs for the rows???? This is important\n",
    "    # This needs to match what we are already doing later, so let's go figure that out FIRST. \n",
    "\n",
    "    # for the leaf values, we want to include ALL parents in the \n",
    "    # loss calculation. So, we initialize the tensor as a ones tensor\n",
    "    # based on the number of leaf values and the number of parents\n",
    "    cell_parents_mask = torch.ones(num_parents,num_leafs)\n",
    "\n",
    "    # now we can deal with the internal values. For these, we will not\n",
    "    # include all parents. We will use parent_dict to select which to include\n",
    "\n",
    "\n",
    "    # first, get a list of all the parents. The ordering of this list\n",
    "    # is used later to propogate probabilities up the ontology.\n",
    "    list_of_parents = ontology_df.index.tolist()\n",
    "\n",
    "    # now, we need to loop through each internal value\n",
    "    # internal_values is ordered as -9999 + n\n",
    "    # this will be helpful later when we need to pull these values out. \n",
    "    # so the columns here are ordered at 0 to (number of leaf values), then -9999\n",
    "    # to (number of internal values)\n",
    "\n",
    "    for cell_id in internal_values:\n",
    "        # get the list of parent cell IDs we want to include for this\n",
    "        # particular internal_values\n",
    "        parent_list_for_cell = parent_dict[cell_id]\n",
    "\n",
    "        # loop through the parent_list_for cell, create a new binary list where\n",
    "        # list is 1 if the parent is in the list_of_parents, otherwise 0\n",
    "        parent_binary_list = [1 if parent in parent_list_for_cell else 0 for parent in list_of_parents]\n",
    "\n",
    "        # convert the list to a tensor and reshape for concatenation\n",
    "        parent_binary_tensor = torch.tensor(parent_binary_list).reshape(-1,1)\n",
    "\n",
    "        # append to cell_parents_to_include. \n",
    "        # we append along columns\n",
    "        cell_parents_mask = torch.cat((cell_parents_mask,parent_binary_tensor),1)\n",
    "\n",
    "    return(cell_parents_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37dcf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_ontology(adata, target_column,upper_limit = None, cl_only = False, include_leafs = False):\n",
    "    '''\n",
    "    This function perfroms preprocessing on ann AnnData object to prepare it for modelling. It will encode the \n",
    "    target column and returns x_data and y_data for modelling\n",
    "    \n",
    "    This function also preprocesses the ontology to build a pandas dataframe that can be used to \n",
    "    calculate predicted probabilities. This will enable simple matrix multiplication to calculate\n",
    "    probabilities and loss.\n",
    "    \n",
    "    Can have an upper limit to the ontology if upper_limit is set\n",
    "    \n",
    "    \n",
    "    Assumes there is an active census object already open as cl. \n",
    "\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : AnnData Object\n",
    "        existing AnnData object to perform processing on \n",
    "        \n",
    "    target_column : string\n",
    "        string of target column (from cell metadata) to encode\n",
    "     \n",
    "    upper_limit : string\n",
    "        if you want to specify an upper limit in the ontology, set this to \n",
    "        the upper limit (inclusive)\n",
    "        Default: None (no limit to ontology)\n",
    "        \n",
    "    cl_only : boolean\n",
    "        option to only include the Cell Ontology (CL) in the dataframe\n",
    "        True means only those cell IDs that start with CL are included\n",
    "        Default: False\n",
    "        \n",
    "    include_leafs : boolean\n",
    "        option to include leafs in the list of parent cell IDs\n",
    "        Default is False because we are calculating the leaf loss differently\n",
    "        Default: False\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_data : SciPy Matrix\n",
    "        scipy sparse CSR matrix\n",
    "    \n",
    "    y_data : Series\n",
    "        Pandas Series of encoded target values\n",
    "        \n",
    "    mapping_dict : Dictionary\n",
    "        dictionary mapping the Cell Ontology IDs (keys) to the encoded values (values)\n",
    "        Values >= 0 are leaf nodes\n",
    "        Values < 0 are internal nodes\n",
    "\n",
    "    leaf_values : list\n",
    "        list composed of all leaf values included in the dataset\n",
    "        includes internal nodes that do not have sub-values in the dataset, and thus are\n",
    "        treated an leaf nodes\n",
    "\n",
    "    internal_values : list\n",
    "        list composed of interanal nodes in the dataset\n",
    "\n",
    "    ontology_df : pandas dataframe\n",
    "        pandas dataframe where indices (rows) are all leaf and parent cell IDs from the portion of \n",
    "        the ontology being queried, and columns are all leafs in portion of ontology being queried. \n",
    "        \n",
    "        Dataframe is binary. For each parent node, element = 1 if parent node is an ancestor\n",
    "        of corresponding leaf node.\n",
    "        \n",
    "    parent_dict : dictionary\n",
    "        keys are internal_values and values are all cell ontology terms within the same distance\n",
    "        from the top node. \n",
    "        \n",
    "    cell_parent_mask : tensor\n",
    "        tensor of shape ik, where i = parent IDs and k = each cell type in the dataset\n",
    "        binary tensor where 1 means for that cell type, that parent ID will be included\n",
    "        in the internal loss calculation\n",
    "        and 0 means for that cell type, that parent ID is excluded in the internal loss\n",
    "        calculation\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # select the labels. \n",
    "    labels = adata.obs\n",
    "    \n",
    "    # encode the target column\n",
    "    #lb = LabelEncoder()\n",
    "    #labels['encoded_labels'] = lb.fit_transform(labels[target_column])\n",
    "    \n",
    "    # we want to only encode the targets that are leafs. We will leave \n",
    "    # internal nodes as the CL number in order to assist with masking \n",
    "    # the appropriate parent nodes \n",
    "    # first, get list of all cell values\n",
    "    all_cell_values = labels[target_column].unique().to_list()\n",
    "    \n",
    "    # identify which values are leafs\n",
    "    # we use positive number for leaf values\n",
    "    # and negative number for internal nodes\n",
    "    mapping_dict = {}\n",
    "    leaf_values = []\n",
    "    internal_values = []\n",
    "    encoded_leaf_val = 0\n",
    "    encoded_internal_val = -9999\n",
    "    for term in all_cell_values:\n",
    "        if cl[term].is_leaf():\n",
    "            mapping_dict[term] = encoded_leaf_val\n",
    "            leaf_values.append(term)\n",
    "            encoded_leaf_val += 1\n",
    "        else:\n",
    "            # check if internal values have associated sub-values in the dataset\n",
    "            #    sub-values do not have to be leafs\n",
    "            # if so, add value as internal values\n",
    "            # if not, prune ontology so consider \n",
    "            term_subvalues = []\n",
    "            # get leaf values of this term\n",
    "            for sub_term in cl[term].subclasses(distance=None,with_self=False).to_set():\n",
    "                    term_subvalues.append(sub_term.id)\n",
    "            \n",
    "            # get values in all_call_values in term_leafs\n",
    "            intersection_list = list(set(all_cell_values).intersection(term_subvalues))\n",
    "            if len(intersection_list) == 0:\n",
    "                mapping_dict[term] = encoded_leaf_val\n",
    "                leaf_values.append(term)\n",
    "                encoded_leaf_val += 1\n",
    "            else:\n",
    "                mapping_dict[term] = encoded_internal_val\n",
    "                internal_values.append(term)\n",
    "                encoded_internal_val += 1            \n",
    "            \n",
    "            \n",
    "    # use the leaf_mapping_dict to \n",
    "    labels['encoded_labels'] = labels[target_column].map(mapping_dict)\n",
    "    \n",
    "    x_data = adata.X.copy()\n",
    "    y_data = labels['encoded_labels']\n",
    "    \n",
    "    #########\n",
    "    # now get a list of all parent nodes for each value in the dataset\n",
    "    # if we want to include leafs, set with_self= True\n",
    "    # else, set with_self = False\n",
    "    \n",
    "    all_parent_nodes = []\n",
    "    for target in all_cell_values:\n",
    "        for term in cl[target].superclasses(distance=None,with_self=include_leafs).to_set():\n",
    "            all_parent_nodes.append(term.id)\n",
    "            #if target == 'CL:0000904':\n",
    "            #    print(term)\n",
    "            \n",
    "    # ensure that we do not have duplicate values\n",
    "    all_parent_nodes = list(set(all_parent_nodes))\n",
    "\n",
    "    # select only the Cell Ontology IDs if cl_only = True\n",
    "    if cl_only:\n",
    "        all_parent_nodes = [x for x in all_parent_nodes if x.startswith('CL')]\n",
    "    \n",
    "    # if there is an upper limit, \n",
    "    if upper_limit is not None:\n",
    "        # get upper limit nodes\n",
    "        upper_limit_nodes = []\n",
    "        for term in cl[upper_limit].superclasses(distance=None,with_self=False).to_set():\n",
    "            upper_limit_nodes.append(term.id)\n",
    "\n",
    "        # remove these nodes from the parent_nodes list\n",
    "        all_parent_nodes = [x for x in all_parent_nodes if x not in upper_limit_nodes]\n",
    "        \n",
    "    # create a dictionary that maps parents to reduce the ontology_df when\n",
    "    # dealing with internal nodes\n",
    "    #parent_dict = set_internal_node_relationships_by_depth(internal_values,upper_limit,all_parent_nodes)\n",
    "    parent_dict = set_internal_node_values(internal_values,all_parent_nodes)\n",
    "    \n",
    "    # create the dataframe\n",
    "    # use all_cell_values for the columns, because we need both leafs and\n",
    "    # internals nodes for mapping\n",
    "    ontology_df = pd.DataFrame(data=0, index = all_parent_nodes,\n",
    "                                              columns = all_cell_values)\n",
    "    \n",
    "    # populate the dataframe with 1 if column is a sub-node \n",
    "    # for that particular cell ID\n",
    "    # with_self = True because we need to include the leafs here\n",
    "    for cell_id in ontology_df.index:\n",
    "        for term in cl[cell_id].subclasses(distance=None,with_self=True).to_set():\n",
    "            if term.id in ontology_df.columns:\n",
    "                ontology_df.loc[cell_id,[term.id]] = [1]\n",
    "\n",
    "    # create a dictionary that maps parents to reduce the ontology_df when\n",
    "    # dealing with internal nodes\n",
    "    #parent_dict = {}\n",
    "    #for parent in internal_values:\n",
    "    #    super_parent_list = []\n",
    "    #    for term in cl[parent].superclasses(distance=None,with_self=True).to_set():\n",
    "    #         if term.id in all_parent_nodes:\n",
    "    #            super_parent_list.append(term.id)\n",
    "    #    parent_dict[parent] = super_parent_list\n",
    "\n",
    "    # build a matrix used to mask parent values\n",
    "    cell_parent_mask = build_parent_mask(leaf_values,internal_values,ontology_df,parent_dict)\n",
    "    \n",
    "    return(x_data,y_data, mapping_dict, leaf_values, internal_values, ontology_df, parent_dict, cell_parent_mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c80c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(x_data):\n",
    "    '''\n",
    "    This function takes the input x_data, transforms the data with log(1+x) and \n",
    "    returns the transformed data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data : scipy matrix\n",
    "        scipy sparse CSR matrix  \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_data : SciPy Matrix\n",
    "        scipy sparse CSR matrix\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # np.log takes the natural log\n",
    "    x_data.data = np.log(1+ x_data.data)\n",
    "\n",
    "    return x_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deacfdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_format_data(x_data, y_data, train_size, val_size, holdout_size = None, random_state = None):\n",
    "    '''\n",
    "    This function splits x_data and y_data into training and validation sets, then formats the data into\n",
    "    tensors for modeling with PyTorch\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data : scipy matrix\n",
    "        scipy sparse CSR matrix  \n",
    "        \n",
    "    y_data : Series\n",
    "        Pandas series of encoded target values\n",
    "        \n",
    "    train_size: float\n",
    "        float between 0.0 and 1.0 to select the training fraction of the data set\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_data : SciPy Matrix\n",
    "        scipy sparse CSR matrix\n",
    "    \n",
    "    y_data : Series\n",
    "        Pandas Series of encoded target values\n",
    "    \n",
    "    X_train : Tensor\n",
    "        pytorch tensor of training values\n",
    "    \n",
    "    X_val : Tensor\n",
    "        pytorch tensor of validation values\n",
    "        \n",
    "    y_train : Tensor\n",
    "        pytorch tensor of training target values\n",
    "        \n",
    "    y_val : Tensor\n",
    "        pytorch tensor of validation target values\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if holdout_size:\n",
    "        # split into training and validation sets\n",
    "        # first split into train and validation/holdout\n",
    "        X_train, X_val_holdout, y_train, y_val_holdout = train_test_split(x_data,y_data,\n",
    "                                                       train_size = train_size,\n",
    "                                                         random_state=random_state)\n",
    "\n",
    "        # calculate the validation split of the remainder \n",
    "        val_split_size = val_size / (val_size + holdout_size)\n",
    "\n",
    "        # split the validation/holdout set to separate sets\n",
    "        X_val, X_holdout, y_val, y_holdout = train_test_split(X_val_holdout, y_val_holdout,\n",
    "                                                             train_size = val_split_size,\n",
    "                                                             random_state=random_state)\n",
    "    else:\n",
    "        # split into training and validation sets\n",
    "        # first split into train and validation/holdout\n",
    "        X_train, X_val, y_train, y_val = train_test_split(x_data,y_data,\n",
    "                                                       train_size = train_size,\n",
    "                                                         random_state=random_state)\n",
    "\n",
    "    \n",
    "    # check if number of genes in X_train = X_val\n",
    "    # rarely, splitting the dataset can cause a difference\n",
    "    # if it does, resplit the data by looping back into this function\n",
    "\n",
    "    if holdout_size:\n",
    "        if (X_train.shape[1] != X_val.shape[1]) or (X_train.shape[1] != X_holdout.shape[1]):\n",
    "            X_train, X_val, y_train, y_val, X_holdout, y_holdout = split_format_data(x_data, y_data, train_size, val_size, holdout_size, random_state = None)\n",
    "        else:\n",
    "            print('Success. Number of genes in datasets match.')\n",
    "    else:\n",
    "        if (X_train.shape[1] != X_val.shape[1]):\n",
    "            X_train, X_val, y_train, y_val, X_holdout, y_holdout = split_format_data(x_data, y_data, train_size, val_size, holdout_size, random_state = None)\n",
    "        else:\n",
    "            print('Success. Number of genes in datasets match.')\n",
    "\n",
    "\n",
    "    \n",
    "    # convert the data to tensors\n",
    "    # we'll change the data from CSR (compressed sparse row) format\n",
    "    # to COO (coordinate) format for better use with pytorch\n",
    "    # see https://pytorch.org/docs/stable/sparse.html for additional details\n",
    "    # conversion from COO to tensor based on https://stackoverflow.com/questions/50665141/converting-a-scipy-coo-matrix-to-pytorch-sparse-tensor\n",
    "\n",
    "    # copy the X matrix to save in scipy CSR format\n",
    "    x_train_csr = X_train.copy()\n",
    "    \n",
    "\n",
    "    X_train_coo = X_train.tocoo()\n",
    "    #X_train = torch.sparse.FloatTensor(torch.LongTensor(np.vstack((X_train_coo.row,X_train_coo.col))),\n",
    "    #                              torch.FloatTensor(X_train_coo.data))\n",
    "\n",
    "    X_train_values = X_train_coo.data\n",
    "    X_train_indices = np.vstack((X_train_coo.row, X_train_coo.col))\n",
    "\n",
    "    X_train_i = torch.LongTensor(X_train_indices)\n",
    "    X_train_v = torch.FloatTensor(X_train_values)\n",
    "    X_train_shape = X_train_coo.shape\n",
    "\n",
    "    X_train = torch.sparse.FloatTensor(X_train_i, X_train_v, torch.Size(X_train_shape))  \n",
    "    \n",
    "    \n",
    "    # y_train is a Series, so it is easier to convert to a tensor\n",
    "    y_train = torch.tensor(y_train,device=device)#, dtype=torch.long)\n",
    "\n",
    "    # and the same for the validation set\n",
    "    X_val_coo = X_val.tocoo()\n",
    "    #X_val = torch.sparse.FloatTensor(torch.LongTensor(np.vstack((X_val_coo.row,X_val_coo.col))),\n",
    "    #                                  torch.FloatTensor(X_val_coo.data))\n",
    "\n",
    "    \n",
    "    X_val_values = X_val_coo.data\n",
    "    X_val_indices = np.vstack((X_val_coo.row, X_val_coo.col))\n",
    "\n",
    "    X_val_i = torch.LongTensor(X_val_indices)\n",
    "    X_val_v = torch.FloatTensor(X_val_values)\n",
    "    X_val_shape = X_val_coo.shape\n",
    "\n",
    "    X_val = torch.sparse.FloatTensor(X_val_i, X_val_v, torch.Size(X_val_shape)).to(device)  \n",
    "    \n",
    "    \n",
    "    # y_val is a Series, so it is easier to convert to a tensor\n",
    "    y_val = torch.tensor(y_val,device=device)#, dtype=torch.long)\n",
    "\n",
    "    if holdout_size:\n",
    "        #pass\n",
    "        # and the same for the holdout set\n",
    "        X_holdout_coo = X_holdout.tocoo()\n",
    "        #X_holdout = torch.sparse.FloatTensor(torch.LongTensor(np.vstack((X_holdout_coo.row,X_holdout_coo.col))),\n",
    "        #                              torch.FloatTensor(X_holdout_coo.data))\n",
    "\n",
    "        X_holdout_values = X_holdout_coo.data\n",
    "        X_holdout_indices = np.vstack((X_holdout_coo.row, X_holdout_coo.col))\n",
    "\n",
    "        X_holdout_i = torch.LongTensor(X_holdout_indices)\n",
    "        X_holdout_v = torch.FloatTensor(X_holdout_values)\n",
    "        X_holdout_shape = X_holdout_coo.shape\n",
    "\n",
    "        X_holdout = torch.sparse.FloatTensor(X_holdout_i, X_holdout_v, torch.Size(X_holdout_shape)).to(device)   \n",
    "\n",
    "        \n",
    "        # y_holdout is a Series, so it is easier to convert to a tensor\n",
    "        y_holdout = torch.tensor(y_holdout, dtype=torch.long,device=device)\n",
    "    \n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        print('y_train, X_val, y_val moved to GPU. X_train not moved to GPU. ')\n",
    "            \n",
    "    if holdout_size:\n",
    "        return(X_train, X_val, y_train, y_val, X_holdout, y_holdout, x_train_csr)\n",
    "    else:\n",
    "        return(X_train, X_val, y_train, y_val,x_train_csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4274b",
   "metadata": {},
   "source": [
    "## Main Loop For Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5605aa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocess data and ontology\n",
      "start transforming data\n",
      "start split and format data\n",
      "Success. Number of genes in datasets match.\n",
      "Preprocessing complete. There are 52 leaf values and 16 internal values.\n",
      "There are 37787 cells in the training set and 9447 cells in the validation set, both contain 19966 genes.\n"
     ]
    }
   ],
   "source": [
    "target_column = 'cell_type_ontology_term_id'\n",
    "\n",
    "upper_limit = 'CL:0000988' # leukocyte = 738, hematopoietic = 988\n",
    "\n",
    "print('start preprocess data and ontology')\n",
    "x_data,y_data, mapping_dict, leaf_values,internal_values, \\\n",
    "    ontology_df, parent_dict, cell_parent_mask =  preprocess_data_ontology(adata, target_column,\n",
    "                                                                           upper_limit = upper_limit, \n",
    "                                                                 cl_only = True, include_leafs = False)\n",
    "\n",
    "###del adata\n",
    "\n",
    "# create dataframe that only includes leaf nodes\n",
    "ontology_leaf_df = ontology_df[leaf_values]\n",
    "\n",
    "print('start transforming data')\n",
    "x_data = transform_data(x_data)\n",
    "\n",
    "\n",
    "train_size = 0.8\n",
    "val_size = 0.2\n",
    "holdout_size =  None # None if you don't want holdout set\n",
    "random_state = 42\n",
    "\n",
    "print('start split and format data')\n",
    "if holdout_size:\n",
    "    X_train, X_val, y_train, y_val, \\\n",
    "    X_holdout, y_holdout, x_train_csr = split_format_data(x_data, y_data, train_size, val_size, \n",
    "                                                          holdout_size, random_state=random_state)\n",
    "else:\n",
    "    X_train, X_val, y_train, y_val, \\\n",
    "    x_train_csr = split_format_data(x_data, y_data, train_size, val_size, \n",
    "                                    holdout_size, random_state=random_state)\n",
    "\n",
    "print('Preprocessing complete. There are {0} leaf values and {1} internal values.'.format(len(leaf_values),len(internal_values)\n",
    "                                                                                         ))\n",
    "print('There are {0} cells in the training set and {1} cells in the validation set, both contain {2} genes.'.format(X_train.shape[0],X_val.shape[0],X_train.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adfa02",
   "metadata": {},
   "source": [
    "## Save Results of Preprocessing to Disk for Use in Modeling\n",
    "\n",
    "- X_train,X_val,y_train,y_val (do this only if we do manual batching. If we switch to DataLoader, we might not want to split the data. Come back to this)\n",
    "- cell_parent_mask\n",
    "- Mapping_dict\n",
    "- Ontology_df\n",
    "- Internal_values\n",
    "- leaf_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to directory you want to save the results in\n",
    "os.chdir('/home/fujoshua/cell_classification')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9984de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get today's date for saving information about this model\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7434d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save information needed for testing external models \n",
    "\n",
    "ontology_df_name = today + '_ontology_df.csv'\n",
    "ontology_df.to_csv(ontology_df_name)\n",
    "\n",
    "mapping_dict_name = today + '_mapping_dict_df.csv'\n",
    "mapping_dict_df = pd.DataFrame.from_dict(mapping_dict,orient='index')\n",
    "mapping_dict_df.to_csv(mapping_dict_name)\n",
    "\n",
    "leaf_values_name = today + '_leaf_values'\n",
    "internal_values_name = today + '_internal_values'\n",
    "\n",
    "with open(leaf_values_name, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(leaf_values, fp)\n",
    "    \n",
    "with open(internal_values_name, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(internal_values, fp)\n",
    "\n",
    "X_train_name = today + '_X_train.pt'\n",
    "X_val_name = today + '_X_val.pt'\n",
    "y_train_name = today + '_y_train.pt'\n",
    "y_val_name = today + '_y_val.pt'\n",
    "\n",
    "torch.save(X_train, X_train_name)\n",
    "torch.save(y_train,y_train_name)\n",
    "torch.save(X_val, X_val_name)\n",
    "torch.save(y_val,y_val_name)\n",
    "    \n",
    "cell_parent_mask_name = today + '_cell_parent_mask.pt'\n",
    "torch.save(cell_parent_mask,cell_parent_mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e74e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d21d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d54d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
